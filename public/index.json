[
{
	"uri": "/workshop/",
	"title": "NLP on AWS",
	"tags": [],
	"description": "",
	"content": "Machine Learning Training using SageMaker Studio Amazon SageMaker Studio is an integrated development environment (IDE) that gives you complete access, control, and visibility into each step required to build, train, and deploy models. You can quickly upload data, create new notebooks, train and tune models, move back and forth between steps to adjust experiments, compare results, and deploy models to production all in one place, making you much more productive..\nAmazon SageMaker makes it easy to train machine learning (ML) models by providing everything you need to tune and debug models and execute training experiments. After you choose the right algorithms and frameworks from the wide range of choices available, it manages all of the underlying infrastructure to train your model at petabyte scale, and deploy it to production.\nIn this workshop you will learn how to use Amazon SageMaker Studio to train your models including:\n How to train a model using an algorithm provided by Amazon SageMaker How to use your custom code (script) to train a model on Amazon SageMaker Studio How to bring your own custom algorithms as containers to run on SageMaker Studio How to track, evaluate, and organize training experiments How to distribute your training job How to tune your model How to get full visibility into the training process through Amazon SageMaker Debugger  "
},
{
	"uri": "/",
	"title": "NLP Workshop",
	"tags": [],
	"description": "",
	"content": "NLP Workshop  Presentation  AWS AI/ML NLP Intro to NLP   Lab   Welcome! Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning (ML) models quickly. SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high quality models.\nWorkshop website: http://bit.ly/nlp\nDay1    Agenda       9:10 AM - 9:30 AM Presentation    9:30 AM - 10:00 AM Lab    10:00 AM - 10:10 AM Presentation    10:10 AM - 10:30 AM Lab    10:30 AM - 10:40 AM  Break   10:40 AM - 10:50 AM Presentation    10:50 AM - 11:50 AM Lab    11:20 AM - 12:00 AM      "
},
{
	"uri": "/workshop/1_gettingstarted/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "Getting Started To start this workshop, if you have not already, you should setup Amazon SageMaker Studio, and then login. In this chapter, we’ll do the following:\n Amazon SageMaker Studio setup Cloning the repository  "
},
{
	"uri": "/workshop/2_builtin/",
	"title": "Built-in Algorithms",
	"tags": [],
	"description": "",
	"content": "Use Amazon SageMaker Built-in Algorithms Amazon SageMaker provides a suite of built-in algorithms to help data scientists and machine learning practitioners get started on training and deploying machine learning models quickly. Learn more\n BlazingText LDA Neuro Topic Modeling (NTM)  "
},
{
	"uri": "/workshop/3_scriptmode/",
	"title": "SageMaker Script Mode",
	"tags": [],
	"description": "",
	"content": "Bring your own model with Amazon SageMaker script mode As the prevalence of machine learning (ML) and artificial intelligence (AI) grows, you need the best mechanisms to aid in the experimentation and development of your algorithms. You might begin with the several built-in algorithms in Amazon SageMaker that simply require you to point the algorithm at your data and start a SageMaker training job. At the other end of the spectrum, you might be quite specialized and have several highly customized algorithms and Docker containers to support those algorithms, and AWS has a workflow to create and support these bespoke components as well. However, it’s increasingly common to have invested time and energy into researching, testing, and building several custom ML algorithms, but use widely used frameworks such as scikit-learn. In this scenario, you don’t need or want to invest the time, money, and resources to create and support bespoke containers.\nTo address this, SageMaker offers a solution using script mode. Script mode enables you to write custom training and inference code while still utilizing common ML framework containers maintained by AWS. Script mode is easy to use and extremely flexible. In this post, we discuss three primary use cases for using script mode, and how script mode can accelerate your algorithm development and testing while simultaneously decreasing the amount of time, effort, and resources required to bring your custom algorithm to the cloud.\nsource: https://aws.amazon.com/blogs/machine-learning/bring-your-own-model-with-amazon-sagemaker-script-mode/\n"
},
{
	"uri": "/workshop/4_byoc/",
	"title": "Hugging Face",
	"tags": [],
	"description": "",
	"content": "Use Hugging Face with Amazon SageMaker Amazon SageMaker enables customers to train, fine-tune, and run inference using Hugging Face models for Natural Language Processing (NLP) on SageMaker. You can use Hugging Face for both training and inference. This functionality is available through the development of Hugging Face AWS Deep Learning Containers. These containers include Hugging Face Transformers, Tokenizers and the Datasets library, which allows you to use these resources for your training and inference jobs. For a list of the available Deep Learning Containers images, see Available Deep Learning Containers Images. These Deep Learning Containers images are maintained and regularly updated with security patches.\nTo use the Hugging Face Deep Learning Containers with the SageMaker Python SDK for training, see the Hugging Face SageMaker Estimator. With the Hugging Face Estimator, you can use the Hugging Face models as you would any other SageMaker Estimator. However, using the SageMaker Python SDK is optional. You can also orchestrate your use of the Hugging Face Deep Learning Containers with the AWS CLI and AWS SDK for Python (Boto3).\nFor more information on Hugging Face and the models available in it, see the Hugging Face documentation.\n"
},
{
	"uri": "/workshop/5_huggingface/",
	"title": "Use a custom Docker image",
	"tags": [],
	"description": "",
	"content": "Bring Your Own Containers Amazon SageMaker\u0026rsquo;s built-in algorithms and supported frameworks should cover most use cases, but there are times when you may need to use an algorithm from a package not included in any of the supported frameworks. You might also have a pre-trained model picked or persisted somewhere which you need to deploy. SageMaker uses Docker images to host the training and serving of all models, so you can supply your own custom Docker image if the package or software you need is not included in a supported framework. This may be your own Python package or an algorithm coded in a language like Stan or Julia. For these images you must also configure the training of the algorithm and serving of the model properly in your Dockerfile. This requires intermediate knowledge of Docker and is not recommended unless you are comfortable writing your own machine learning algorithm. Your Docker image must be uploaded to an online repository, such as the Amazon Elastic Container Registry (ECR) before you can train and serve your model properly.\nFor more information on custom Docker images in SageMaker, see Using Docker containers with SageMaker.\nsource: https://docs.aws.amazon.com/sagemaker/latest/dg/algorithms-choose.html#custom-image-use-case\n"
},
{
	"uri": "/workshop/1_gettingstarted/create/",
	"title": "Setup",
	"tags": [],
	"description": "",
	"content": "  Login into Amazon SageMaker\n  Select a Region.\nAmazon SageMaker Studio is available in the following AWS Regions:\n US East (Ohio), us-east-2 US East (N. Virginia), us-east-1 US West (N. Oregon), us-west-2 China (Beijing), cn-north-1 China (Ningxia), cn-northwest-1 EU (Ireland), eu-west-1  To change the Region in the Amazon SageMaker console, use the Region selector at the upper-right corner of the console.\n  Click on Amazon SageMaker Studio and complete the Studio onboarding process using the Amazon SageMaker console.\nWhen onboarding, you can choose to use either AWS Single Sign-On (AWS SSO) or AWS Identity and Access Management (IAM) for authentication methods. When you use IAM authentication, you can choose either the Quick start or the Standard setup procedure.\n The simplest way to create an Amazon SageMaker Studio account is to follow the Quick start procedure. For more control, including the option of using AWS SSO authentication, use the Standard setup procedures. Follow this link to create a notebook instance:    login into the studio either through AWS SSO authentication or with IAM authentication.\n  In the next step, we will load the notebooks into Amazon SageMaker Studio.\n"
},
{
	"uri": "/workshop/3_scriptmode/pytorch_rnn/",
	"title": "Word-level language modeling using PyTorch",
	"tags": [],
	"description": "",
	"content": "Contents  Background Setup Data Train Host   Background This example trains a multi-layer LSTM RNN model on a language modeling task based on PyTorch example. By default, the training script uses the Wikitext-2 dataset. We will train a model on SageMaker, deploy it, and then use deployed model to generate new text.\nFor more information about the PyTorch in SageMaker, please visit sagemaker-pytorch-containers and sagemaker-python-sdk github repositories.\n Setup This notebook was created and tested on an ml.p2.xlarge notebook instance.\nLet\u0026rsquo;s start by creating a SageMaker session and specifying:\n The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting. The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the sagemaker.get_execution_role() with appropriate full IAM role arn string(s).  import sagemaker sagemaker_session = sagemaker.Session() bucket = sagemaker_session.default_bucket() prefix = \u0026#34;sagemaker/DEMO-pytorch-rnn-lstm\u0026#34; role = sagemaker.get_execution_role() Data Getting the data As mentioned above we are going to use the wikitext-2 raw data. This data is from Wikipedia and is licensed CC-BY-SA-3.0. Before you use this data for any other purpose than this example, you should understand the data license, described at https://creativecommons.org/licenses/by-sa/3.0/\n%%bash wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip unzip -n wikitext-2-raw-v1.zip cd wikitext-2-raw mv wiki.test.raw test \u0026amp;\u0026amp; mv wiki.train.raw train \u0026amp;\u0026amp; mv wiki.valid.raw valid Let\u0026rsquo;s preview what data looks like.\n!head -5 wikitext-2-raw/train Uploading the data to S3 We are going to use the sagemaker.Session.upload_data function to upload our datasets to an S3 location. The return value inputs identifies the location \u0026ndash; we will use later when we start the training job.\ninputs = sagemaker_session.upload_data(path=\u0026#34;wikitext-2-raw\u0026#34;, bucket=bucket, key_prefix=prefix) print(\u0026#34;input spec (in this case, just an S3 path): {}\u0026#34;.format(inputs)) Train Training script We need to provide a training script that can run on the SageMaker platform. The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, such as:\n SM_MODEL_DIR: A string representing the path to the directory to write model artifacts to. These artifacts are uploaded to S3 for model hosting. SM_OUTPUT_DATA_DIR: A string representing the filesystem path to write output artifacts to. Output artifacts may include checkpoints, graphs, and other files to save, not including model artifacts. These artifacts are compressed and uploaded to S3 to the same S3 prefix as the model artifacts.  Supposing one input channel, \u0026lsquo;training\u0026rsquo;, was used in the call to the PyTorch estimator\u0026rsquo;s fit() method, the following will be set, following the format SM_CHANNEL_[channel_name]:\n SM_CHANNEL_TRAINING: A string representing the path to the directory containing data in the \u0026lsquo;training\u0026rsquo; channel.  A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to model_dir so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an argparse.ArgumentParser instance.\nIn this notebook example, we will use Git integration. That is, you can specify a training script that is stored in a GitHub, CodeCommit or other Git repository as the entry point for the estimator, so that you don\u0026rsquo;t have to download the scripts locally. If you do so, source directory and dependencies should be in the same repo if they are needed.\nTo use Git integration, pass a dict git_config as a parameter when you create the PyTorch Estimator object. In the git_config parameter, you specify the fields repo, branch and commit to locate the specific repo you want to use. If authentication is required to access the repo, you can specify fields 2FA_enabled, username, password and token accordingly.\nThe script that we will use in this example is stored in GitHub repo https://github.com/awslabs/amazon-sagemaker-examples/tree/training-scripts, under the branch training-scripts. It is a public repo so we don\u0026rsquo;t need authentication to access it. Let\u0026rsquo;s specify the git_config argument here:\ngit_config = { \u0026#34;repo\u0026#34;: \u0026#34;https://github.com/awslabs/amazon-sagemaker-examples.git\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;training-scripts\u0026#34;, } Note that we do not specify commit in git_config here, in which case the latest commit of the specified repo and branch will be used by default.\nA typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to model_dir so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an argparse.ArgumentParser instance.\nFor example, the script run by this notebook: https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/train.py.\nFor more information about training environment variables, please visit SageMaker Containers.\nIn the current example we also need to provide source directory, because training script imports data and model classes from other modules. The source directory is https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/. We should provide \u0026lsquo;pytorch-rnn-scripts\u0026rsquo; for source_dir when creating the Estimator object, which is a relative path inside the Git repository.\nRun training in SageMaker The PyTorch class allows us to run our training function as a training job on SageMaker infrastructure. We need to configure it with our training script and source directory, an IAM role, the number of training instances, and the training instance type. In this case we will run our training job on ml.p2.xlarge instance. As you can see in this example you can also specify hyperparameters.\nFor this example, we\u0026rsquo;re specifying the number of epochs to be 1 for the purposes of demonstration. We suggest at least 6 epochs for a more meaningful result.\nfrom sagemaker.pytorch import PyTorch estimator = PyTorch( entry_point=\u0026#34;train.py\u0026#34;, role=role, framework_version=\u0026#34;1.4.0\u0026#34;, instance_count=1, instance_type=\u0026#34;ml.p2.xlarge\u0026#34;, source_dir=\u0026#34;pytorch-rnn-scripts\u0026#34;, py_version=\u0026#34;py3\u0026#34;, git_config=git_config, # available hyperparameters: emsize, nhid, nlayers, lr, clip, epochs, batch_size, # bptt, dropout, tied, seed, log_interval hyperparameters={\u0026#34;epochs\u0026#34;: 1, \u0026#34;tied\u0026#34;: True}, ) After we\u0026rsquo;ve constructed our PyTorch object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk.\nestimator.fit({\u0026#34;training\u0026#34;: inputs}) Host Hosting script We are going to provide custom implementation of model_fn, input_fn, output_fn and predict_fn hosting functions in a separate file, which is in the same Git repo as the training script: https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/generate.py. We will use Git integration for hosting too since the hosting code is also in the Git repo.\nYou can also put your training and hosting code in the same file but you would need to add a main guard (if __name__=='__main__':) for the training code, so that the container does not inadvertently run it at the wrong point in execution during hosting.\nImport model into SageMaker The PyTorch model uses a npy serializer and deserializer by default. For this example, since we have a custom implementation of all the hosting functions and plan on using JSON instead, we need a predictor that can serialize and deserialize JSON.\nfrom sagemaker.predictor import Predictor from sagemaker.serializers import JSONSerializer from sagemaker.deserializers import JSONDeserializer class JSONPredictor(Predictor): def __init__(self, endpoint_name, sagemaker_session): super(JSONPredictor, self).__init__( endpoint_name, sagemaker_session, JSONSerializer(), JSONDeserializer() ) Since hosting functions implemented outside of train script we can\u0026rsquo;t just use estimator object to deploy the model. Instead we need to create a PyTorchModel object using the latest training job to get the S3 location of the trained model data. Besides model data location in S3, we also need to configure PyTorchModel with the script and source directory (because our generate script requires model and data classes from source directory), an IAM role.\nfrom sagemaker.pytorch import PyTorchModel training_job_name = estimator.latest_training_job.name desc = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=training_job_name) trained_model_location = desc[\u0026#34;ModelArtifacts\u0026#34;][\u0026#34;S3ModelArtifacts\u0026#34;] model = PyTorchModel( model_data=trained_model_location, role=role, framework_version=\u0026#34;1.0.0\u0026#34;, entry_point=\u0026#34;generate.py\u0026#34;, source_dir=\u0026#34;pytorch-rnn-scripts\u0026#34;, py_version=\u0026#34;py3\u0026#34;, git_config=git_config, predictor_cls=JSONPredictor, ) Create endpoint Now the model is ready to be deployed at a SageMaker endpoint and we are going to use the sagemaker.pytorch.model.PyTorchModel.deploy method to do this. We can use a CPU-based instance for inference (in this case an ml.m4.xlarge), even though we trained on GPU instances, because at the end of training we moved model to cpu before returning it. This way we can load trained model on any device and then move to GPU if CUDA is available.\npredictor = model.deploy(initial_instance_count=1, instance_type=\u0026#34;ml.m4.xlarge\u0026#34;) Evaluate We are going to use our deployed model to generate text by providing random seed, temperature (higher will increase diversity) and number of words we would like to get.\ninput = {\u0026#34;seed\u0026#34;: 111, \u0026#34;temperature\u0026#34;: 2.0, \u0026#34;words\u0026#34;: 100} response = predictor.predict(input) print(response) Cleanup After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it.\npredictor.delete_endpoint() "
},
{
	"uri": "/workshop/4_byoc/pytorch_rnn/",
	"title": "Word-level language modeling using PyTorch",
	"tags": [],
	"description": "",
	"content": "Contents  Background Setup Data Train Host   Background This example trains a multi-layer LSTM RNN model on a language modeling task based on PyTorch example. By default, the training script uses the Wikitext-2 dataset. We will train a model on SageMaker, deploy it, and then use deployed model to generate new text.\nFor more information about the PyTorch in SageMaker, please visit sagemaker-pytorch-containers and sagemaker-python-sdk github repositories.\n Setup This notebook was created and tested on an ml.p2.xlarge notebook instance.\nLet\u0026rsquo;s start by creating a SageMaker session and specifying:\n The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting. The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the sagemaker.get_execution_role() with appropriate full IAM role arn string(s).  import sagemaker sagemaker_session = sagemaker.Session() bucket = sagemaker_session.default_bucket() prefix = \u0026#34;sagemaker/DEMO-pytorch-rnn-lstm\u0026#34; role = sagemaker.get_execution_role() Data Getting the data As mentioned above we are going to use the wikitext-2 raw data. This data is from Wikipedia and is licensed CC-BY-SA-3.0. Before you use this data for any other purpose than this example, you should understand the data license, described at https://creativecommons.org/licenses/by-sa/3.0/\n%%bash wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip unzip -n wikitext-2-raw-v1.zip cd wikitext-2-raw mv wiki.test.raw test \u0026amp;\u0026amp; mv wiki.train.raw train \u0026amp;\u0026amp; mv wiki.valid.raw valid Let\u0026rsquo;s preview what data looks like.\n!head -5 wikitext-2-raw/train Uploading the data to S3 We are going to use the sagemaker.Session.upload_data function to upload our datasets to an S3 location. The return value inputs identifies the location \u0026ndash; we will use later when we start the training job.\ninputs = sagemaker_session.upload_data(path=\u0026#34;wikitext-2-raw\u0026#34;, bucket=bucket, key_prefix=prefix) print(\u0026#34;input spec (in this case, just an S3 path): {}\u0026#34;.format(inputs)) Train Training script We need to provide a training script that can run on the SageMaker platform. The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, such as:\n SM_MODEL_DIR: A string representing the path to the directory to write model artifacts to. These artifacts are uploaded to S3 for model hosting. SM_OUTPUT_DATA_DIR: A string representing the filesystem path to write output artifacts to. Output artifacts may include checkpoints, graphs, and other files to save, not including model artifacts. These artifacts are compressed and uploaded to S3 to the same S3 prefix as the model artifacts.  Supposing one input channel, \u0026lsquo;training\u0026rsquo;, was used in the call to the PyTorch estimator\u0026rsquo;s fit() method, the following will be set, following the format SM_CHANNEL_[channel_name]:\n SM_CHANNEL_TRAINING: A string representing the path to the directory containing data in the \u0026lsquo;training\u0026rsquo; channel.  A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to model_dir so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an argparse.ArgumentParser instance.\nIn this notebook example, we will use Git integration. That is, you can specify a training script that is stored in a GitHub, CodeCommit or other Git repository as the entry point for the estimator, so that you don\u0026rsquo;t have to download the scripts locally. If you do so, source directory and dependencies should be in the same repo if they are needed.\nTo use Git integration, pass a dict git_config as a parameter when you create the PyTorch Estimator object. In the git_config parameter, you specify the fields repo, branch and commit to locate the specific repo you want to use. If authentication is required to access the repo, you can specify fields 2FA_enabled, username, password and token accordingly.\nThe script that we will use in this example is stored in GitHub repo https://github.com/awslabs/amazon-sagemaker-examples/tree/training-scripts, under the branch training-scripts. It is a public repo so we don\u0026rsquo;t need authentication to access it. Let\u0026rsquo;s specify the git_config argument here:\ngit_config = { \u0026#34;repo\u0026#34;: \u0026#34;https://github.com/awslabs/amazon-sagemaker-examples.git\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;training-scripts\u0026#34;, } Note that we do not specify commit in git_config here, in which case the latest commit of the specified repo and branch will be used by default.\nA typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to model_dir so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an argparse.ArgumentParser instance.\nFor example, the script run by this notebook: https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/train.py.\nFor more information about training environment variables, please visit SageMaker Containers.\nIn the current example we also need to provide source directory, because training script imports data and model classes from other modules. The source directory is https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/. We should provide \u0026lsquo;pytorch-rnn-scripts\u0026rsquo; for source_dir when creating the Estimator object, which is a relative path inside the Git repository.\nRun training in SageMaker The PyTorch class allows us to run our training function as a training job on SageMaker infrastructure. We need to configure it with our training script and source directory, an IAM role, the number of training instances, and the training instance type. In this case we will run our training job on ml.p2.xlarge instance. As you can see in this example you can also specify hyperparameters.\nFor this example, we\u0026rsquo;re specifying the number of epochs to be 1 for the purposes of demonstration. We suggest at least 6 epochs for a more meaningful result.\nfrom sagemaker.pytorch import PyTorch estimator = PyTorch( entry_point=\u0026#34;train.py\u0026#34;, role=role, framework_version=\u0026#34;1.4.0\u0026#34;, instance_count=1, instance_type=\u0026#34;ml.p2.xlarge\u0026#34;, source_dir=\u0026#34;pytorch-rnn-scripts\u0026#34;, py_version=\u0026#34;py3\u0026#34;, git_config=git_config, # available hyperparameters: emsize, nhid, nlayers, lr, clip, epochs, batch_size, # bptt, dropout, tied, seed, log_interval hyperparameters={\u0026#34;epochs\u0026#34;: 1, \u0026#34;tied\u0026#34;: True}, ) After we\u0026rsquo;ve constructed our PyTorch object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk.\nestimator.fit({\u0026#34;training\u0026#34;: inputs}) Host Hosting script We are going to provide custom implementation of model_fn, input_fn, output_fn and predict_fn hosting functions in a separate file, which is in the same Git repo as the training script: https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/generate.py. We will use Git integration for hosting too since the hosting code is also in the Git repo.\nYou can also put your training and hosting code in the same file but you would need to add a main guard (if __name__=='__main__':) for the training code, so that the container does not inadvertently run it at the wrong point in execution during hosting.\nImport model into SageMaker The PyTorch model uses a npy serializer and deserializer by default. For this example, since we have a custom implementation of all the hosting functions and plan on using JSON instead, we need a predictor that can serialize and deserialize JSON.\nfrom sagemaker.predictor import Predictor from sagemaker.serializers import JSONSerializer from sagemaker.deserializers import JSONDeserializer class JSONPredictor(Predictor): def __init__(self, endpoint_name, sagemaker_session): super(JSONPredictor, self).__init__( endpoint_name, sagemaker_session, JSONSerializer(), JSONDeserializer() ) Since hosting functions implemented outside of train script we can\u0026rsquo;t just use estimator object to deploy the model. Instead we need to create a PyTorchModel object using the latest training job to get the S3 location of the trained model data. Besides model data location in S3, we also need to configure PyTorchModel with the script and source directory (because our generate script requires model and data classes from source directory), an IAM role.\nfrom sagemaker.pytorch import PyTorchModel training_job_name = estimator.latest_training_job.name desc = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=training_job_name) trained_model_location = desc[\u0026#34;ModelArtifacts\u0026#34;][\u0026#34;S3ModelArtifacts\u0026#34;] model = PyTorchModel( model_data=trained_model_location, role=role, framework_version=\u0026#34;1.0.0\u0026#34;, entry_point=\u0026#34;generate.py\u0026#34;, source_dir=\u0026#34;pytorch-rnn-scripts\u0026#34;, py_version=\u0026#34;py3\u0026#34;, git_config=git_config, predictor_cls=JSONPredictor, ) Create endpoint Now the model is ready to be deployed at a SageMaker endpoint and we are going to use the sagemaker.pytorch.model.PyTorchModel.deploy method to do this. We can use a CPU-based instance for inference (in this case an ml.m4.xlarge), even though we trained on GPU instances, because at the end of training we moved model to cpu before returning it. This way we can load trained model on any device and then move to GPU if CUDA is available.\npredictor = model.deploy(initial_instance_count=1, instance_type=\u0026#34;ml.m4.xlarge\u0026#34;) Evaluate We are going to use our deployed model to generate text by providing random seed, temperature (higher will increase diversity) and number of words we would like to get.\ninput = {\u0026#34;seed\u0026#34;: 111, \u0026#34;temperature\u0026#34;: 2.0, \u0026#34;words\u0026#34;: 100} response = predictor.predict(input) print(response) Cleanup After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it.\npredictor.delete_endpoint() "
},
{
	"uri": "/workshop/1_gettingstarted/clone/",
	"title": "Clone the Repository",
	"tags": [],
	"description": "",
	"content": "Now, let\u0026rsquo;s open a Terminal in SageMaker studio by going to File \u0026gt; New \u0026gt; Terminal and perform the following commands\ngit clone https://github.com/saeedaghabozorgi/MLAI.git On the left sidebar, we\u0026rsquo;ll see a new folder called \u0026ldquo;MLAI\u0026rdquo;\n"
},
{
	"uri": "/workshop/5_huggingface/hf_spot_notebook/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Huggingface Sagemaker-sdk - Spot instances example Binary Classification with Trainer and imdb dataset  Introduction Development Environment and Permissions  Installation Development environment Permissions   Processing  Tokenization Uploading data to sagemaker_session_bucket   Fine-tuning \u0026amp; starting Sagemaker Training Job  Creating an Estimator and start a training job Estimator Parameters Download fine-tuned model from s3 Attach to old training job to an estimator    Coming soon:Push model to the Hugging Face hub  Introduction Welcome to our end-to-end binary Text-Classification example. In this demo, we will use the Hugging Faces transformers and datasets library together with a custom Amazon sagemaker-sdk extension to fine-tune a pre-trained transformer on binary text classification. In particular, the pre-trained model will be fine-tuned using the imdb dataset. To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on. This demo will also show you can use spot instances and continue training.\nNOTE: You can run this demo in Sagemaker Studio, your local machine or Sagemaker Notebook Instances\nDevelopment Environment and Permissions Installation Note: we only install the required libraries from Hugging Face and AWS. You also need PyTorch or Tensorflow, if you haven´t it installed\n!pip install \u0026#34;sagemaker\u0026gt;=2.48.0\u0026#34; \u0026#34;transformers==4.6.1\u0026#34; \u0026#34;datasets[s3]==1.6.2\u0026#34; --upgrade Development environment upgrade ipywidgets for datasets library and restart kernel, only needed when prerpocessing is done in the notebook\n%%capture import IPython !conda install -c conda-forge ipywidgets -y IPython.Application.instance().kernel.do_shutdown(True) # has to restart kernel so changes are used import sagemaker.huggingface Permissions If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find here more about it.\nimport sagemaker sess = sagemaker.Session() # sagemaker session bucket -\u0026gt; used for uploading data, models and logs # sagemaker will automatically create this bucket if it not exists sagemaker_session_bucket=None if sagemaker_session_bucket is None and sess is not None: # set to default bucket if a bucket name is not given sagemaker_session_bucket = sess.default_bucket() role = sagemaker.get_execution_role() sess = sagemaker.Session(default_bucket=sagemaker_session_bucket) print(f\u0026#34;sagemaker role arn: {role}\u0026#34;) print(f\u0026#34;sagemaker bucket: {sess.default_bucket()}\u0026#34;) print(f\u0026#34;sagemaker session region: {sess.boto_region_name}\u0026#34;) Preprocessing We are using the datasets library to download and preprocess the imdb dataset. After preprocessing, the dataset will be uploaded to our sagemaker_session_bucket to be used within our training job. The imdb dataset consists of 25000 training and 25000 testing highly polar movie reviews.\nTokenization from datasets import load_dataset from transformers import AutoTokenizer # tokenizer used in preprocessing tokenizer_name = \u0026#39;distilbert-base-uncased\u0026#39; # dataset used dataset_name = \u0026#39;imdb\u0026#39; # s3 key prefix for the data s3_prefix = \u0026#39;samples/datasets/imdb\u0026#39; # load dataset dataset = load_dataset(dataset_name) # download tokenizer tokenizer = AutoTokenizer.from_pretrained(tokenizer_name) # tokenizer helper function def tokenize(batch): return tokenizer(batch[\u0026#39;text\u0026#39;], padding=\u0026#39;max_length\u0026#39;, truncation=True) # load dataset train_dataset, test_dataset = load_dataset(\u0026#39;imdb\u0026#39;, split=[\u0026#39;train\u0026#39;, \u0026#39;test\u0026#39;]) test_dataset = test_dataset.shuffle().select(range(10000)) # smaller the size for test dataset to 10k  # tokenize dataset train_dataset = train_dataset.map(tokenize, batched=True) test_dataset = test_dataset.map(tokenize, batched=True) # set format for pytorch train_dataset = train_dataset.rename_column(\u0026#34;label\u0026#34;, \u0026#34;labels\u0026#34;) train_dataset.set_format(\u0026#39;torch\u0026#39;, columns=[\u0026#39;input_ids\u0026#39;, \u0026#39;attention_mask\u0026#39;, \u0026#39;labels\u0026#39;]) test_dataset = test_dataset.rename_column(\u0026#34;label\u0026#34;, \u0026#34;labels\u0026#34;) test_dataset.set_format(\u0026#39;torch\u0026#39;, columns=[\u0026#39;input_ids\u0026#39;, \u0026#39;attention_mask\u0026#39;, \u0026#39;labels\u0026#39;]) Uploading data to sagemaker_session_bucket After we processed the datasets we are going to use the new FileSystem integration to upload our dataset to S3.\nimport botocore from datasets.filesystems import S3FileSystem s3 = S3FileSystem() # save train_dataset to s3 training_input_path = f\u0026#39;s3://{sess.default_bucket()}/{s3_prefix}/train\u0026#39; train_dataset.save_to_disk(training_input_path,fs=s3) # save test_dataset to s3 test_input_path = f\u0026#39;s3://{sess.default_bucket()}/{s3_prefix}/test\u0026#39; test_dataset.save_to_disk(test_input_path,fs=s3) training_input_path = f\u0026#39;s3://{sess.default_bucket()}/{s3_prefix}/train\u0026#39; test_input_path = f\u0026#39;s3://{sess.default_bucket()}/{s3_prefix}/test\u0026#39; Fine-tuning \u0026amp; starting Sagemaker Training Job In order to create a sagemaker training job we need an HuggingFace Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as entry_point, which instance_type should be used, which hyperparameters are passed in \u0026hellip;..\nhuggingface_estimator = HuggingFace(entry_point=\u0026#39;train.py\u0026#39;, source_dir=\u0026#39;./scripts\u0026#39;, base_job_name=\u0026#39;huggingface-sdk-extension\u0026#39;, instance_type=\u0026#39;ml.p3.2xlarge\u0026#39;, instance_count=1, transformers_version=\u0026#39;4.4\u0026#39;, pytorch_version=\u0026#39;1.6\u0026#39;, py_version=\u0026#39;py36\u0026#39;, role=role, hyperparameters = {\u0026#39;epochs\u0026#39;: 1, \u0026#39;train_batch_size\u0026#39;: 32, \u0026#39;model_name\u0026#39;:\u0026#39;distilbert-base-uncased\u0026#39; }) When we create a SageMaker training job, SageMaker takes care of starting and managing all the required ec2 instances for us with the huggingface container, uploads the provided fine-tuning script train.py and downloads the data from our sagemaker_session_bucket into the container at /opt/ml/input/data. Then, it starts the training job by running.\n/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32 The hyperparameters you define in the HuggingFace estimator are passed in as named arguments.\nSagemaker is providing useful properties about the training environment through various environment variables, including the following:\n  SM_MODEL_DIR: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting.\n  SM_NUM_GPUS: An integer representing the number of GPUs available to the host.\n  SM_CHANNEL_XXXX: A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimator’s fit call, named train and test, the environment variables SM_CHANNEL_TRAIN and SM_CHANNEL_TEST are set.\n  To run your training job locally you can define instance_type='local' or instance_type='local-gpu' for gpu usage. Note: this does not working within SageMaker Studio\n!pygmentize ./scripts/train.py \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AutoModelForSequenceClassification, Trainer, TrainingArguments \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtrainer_utils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m get_last_checkpoint \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m accuracy_score, precision_recall_fscore_support \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_from_disk \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m \u001b[37m# Set up logging\u001b[39;49;00m logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m) logging.basicConfig( level=logging.getLevelName(\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33mINFO\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m), handlers=[logging.StreamHandler(sys.stdout)], \u001b[36mformat\u001b[39;49;00m=\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33m%(asctime)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(name)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(levelname)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(message)s\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m, ) \u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m: logger.info(sys.argv) parser = argparse.ArgumentParser() \u001b[37m# hyperparameters sent by the client are passed as command-line arguments to the script.\u001b[39;49;00m parser.add_argument(\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m3\u001b[39;49;00m) parser.add_argument(\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33m--train-batch-size\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m32\u001b[39;49;00m) parser.add_argument(\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33m--eval-batch-size\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m) parser.add_argument(\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33m--warmup_steps\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m500\u001b[39;49;00m) parser.add_argument(\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33m--model_name\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m) parser.add_argument(\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34m5e-5\u001b[39;49;00m) parser.add_argument(\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33m--output_dir\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m) \u001b[37m# Data, model, and output directories\u001b[39;49;00m parser.add_argument(\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33m--output-data-dir\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m]) parser.add_argument(\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m]) parser.add_argument(\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33m--n_gpus\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m]) parser.add_argument(\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33m--training_dir\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m]) parser.add_argument(\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33m--test_dir\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m]) args, _ = parser.parse_known_args() \u001b[37m# load datasets\u001b[39;49;00m train_dataset = load_from_disk(args.training_dir) test_dataset = load_from_disk(args.test_dir) logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33m loaded train_dataset length is: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mlen\u001b[39;49;00m(train_dataset)\u001b[33m}\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m) logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33m loaded test_dataset length is: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mlen\u001b[39;49;00m(test_dataset)\u001b[33m}\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m) \u001b[37m# compute metrics function for binary classification\u001b[39;49;00m \u001b[34mdef\u001b[39;49;00m \u001b[32mcompute_metrics\u001b[39;49;00m(pred): labels = pred.label_ids preds = pred.predictions.argmax(-\u001b[34m1\u001b[39;49;00m) precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33mbinary\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m) acc = accuracy_score(labels, preds) \u001b[34mreturn\u001b[39;49;00m {\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m: acc, \u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33mf1\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m: f1, \u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33mprecision\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m: precision, \u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33mrecall\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m: recall} \u001b[37m# download model from model hub\u001b[39;49;00m model = AutoModelForSequenceClassification.from_pretrained(args.model_name) \u001b[37m# define training args\u001b[39;49;00m training_args = TrainingArguments( output_dir=args.output_dir, num_train_epochs=args.epochs, per_device_train_batch_size=args.train_batch_size, per_device_eval_batch_size=args.eval_batch_size, warmup_steps=args.warmup_steps, evaluation_strategy=\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m, logging_dir=\u001b[33mf\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.output_data_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/logs\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m, learning_rate=\u001b[36mfloat\u001b[39;49;00m(args.learning_rate), ) \u001b[37m# create Trainer instance\u001b[39;49;00m trainer = Trainer( model=model, args=training_args, compute_metrics=compute_metrics, train_dataset=train_dataset, eval_dataset=test_dataset, ) \u001b[37m# train model\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m get_last_checkpoint(args.output_dir) \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m: logger.info(\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33m***** continue training *****\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m) trainer.train(resume_from_checkpoint=args.output_dir) \u001b[34melse\u001b[39;49;00m: trainer.train() \u001b[37m# evaluate model\u001b[39;49;00m eval_result = trainer.evaluate(eval_dataset=test_dataset) \u001b[37m# writes eval result to file which can be accessed later in s3 ouput\u001b[39;49;00m \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.output_data_dir, \u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33meval_results.txt\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m), \u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m writer: \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33m***** Eval results *****\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m key, value \u001b[35min\u001b[39;49;00m \u001b[36msorted\u001b[39;49;00m(eval_result.items()): writer.write(\u001b[33mf\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mkey\u001b[33m}\u001b[39;49;00m\u001b[33m = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mvalue\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\u0026quot;\u001b[39;49;00m) \u001b[37m# Saves the model to s3\u001b[39;49;00m trainer.save_model(args.model_dir)  Creating an Estimator and start a training job from sagemaker.huggingface import HuggingFace # hyperparameters, which are passed into the training job hyperparameters={\u0026#39;epochs\u0026#39;: 1, \u0026#39;train_batch_size\u0026#39;: 32, \u0026#39;model_name\u0026#39;:\u0026#39;distilbert-base-uncased\u0026#39;, \u0026#39;output_dir\u0026#39;:\u0026#39;/opt/ml/checkpoints\u0026#39; } # s3 uri where our checkpoints will be uploaded during training job_name = \u0026#34;using-spot\u0026#34; checkpoint_s3_uri = f\u0026#39;s3://{sess.default_bucket()}/{job_name}/checkpoints\u0026#39; huggingface_estimator = HuggingFace(entry_point=\u0026#39;train.py\u0026#39;, source_dir=\u0026#39;./scripts\u0026#39;, instance_type=\u0026#39;ml.p3.2xlarge\u0026#39;, instance_count=1, base_job_name=job_name, checkpoint_s3_uri=checkpoint_s3_uri, use_spot_instances=True, max_wait=3600, # This should be equal to or greater than max_run in seconds\u0026#39; max_run=1000, # expected max run in seconds role=role, transformers_version=\u0026#39;4.6\u0026#39;, pytorch_version=\u0026#39;1.7\u0026#39;, py_version=\u0026#39;py36\u0026#39;, hyperparameters = hyperparameters) # starting the train job with our uploaded datasets as input huggingface_estimator.fit({\u0026#39;train\u0026#39;: training_input_path, \u0026#39;test\u0026#39;: test_input_path}) # Training seconds: 874 # Billable seconds: 262 # Managed Spot Training savings: 70.0% Deploying the endpoint To deploy our endpoint, we call deploy() on our HuggingFace estimator object, passing in our desired number of instances and instance type.\npredictor = huggingface_estimator.deploy(1,\u0026#34;ml.g4dn.xlarge\u0026#34;) Then, we use the returned predictor object to call the endpoint.\nsentiment_input= {\u0026#34;inputs\u0026#34;:\u0026#34;I love using the new Inference DLC.\u0026#34;} predictor.predict(sentiment_input) Finally, we delete the endpoint again.\npredictor.delete_endpoint() Extras Estimator Parameters # container image used for training job print(f\u0026#34;container image used for training job: \\n{huggingface_estimator.image_uri}\\n\u0026#34;) # s3 uri where the trained model is located print(f\u0026#34;s3 uri where the trained model is located: \\n{huggingface_estimator.model_data}\\n\u0026#34;) # latest training job name for this estimator print(f\u0026#34;latest training job name for this estimator: \\n{huggingface_estimator.latest_training_job.name}\\n\u0026#34;) container image used for training job: 558105141721.dkr.ecr.us-east-1.amazonaws.com/huggingface-training:pytorch1.6.0-transformers4.2.2-tokenizers0.9.4-datasets1.2.1-py36-gpu-cu110 s3 uri where the trained model is located: s3://philipps-sagemaker-bucket-us-east-1/huggingface-training-2021-02-04-16-47-39-189/output/model.tar.gz latest training job name for this estimator: huggingface-training-2021-02-04-16-47-39-189  # access the logs of the training job huggingface_estimator.sagemaker_session.logs_for_job(huggingface_estimator.latest_training_job.name) Attach to old training job to an estimator In Sagemaker you can attach an old training job to an estimator to continue training, get results etc..\nfrom sagemaker.estimator import Estimator # job which is going to be attached to the estimator old_training_job_name=\u0026#39;\u0026#39; # attach old training job huggingface_estimator_loaded = Estimator.attach(old_training_job_name) # get model output s3 from training job huggingface_estimator_loaded.model_data 2021-01-15 19:31:50 Starting - Preparing the instances for training 2021-01-15 19:31:50 Downloading - Downloading input data 2021-01-15 19:31:50 Training - Training image download completed. Training in progress. 2021-01-15 19:31:50 Uploading - Uploading generated training model 2021-01-15 19:31:50 Completed - Training job completed 's3://philipps-sagemaker-bucket-eu-central-1/huggingface-sdk-extension-2021-01-15-19-14-13-725/output/model.tar.gz'  "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/authors/",
	"title": "Credits",
	"tags": [],
	"description": "",
	"content": "   "
},
{
	"uri": "/more_resources/",
	"title": "More Resources",
	"tags": [],
	"description": "",
	"content": "Discover more AWS resources for building and running Machine Learnig projects on Amazon SageMaker:\n  AWS AI/ML NLP\n  Intro to NLP\n  More Hands on Labs\n  Developer resources\n  Youtube - Technical Deep Dive Playlist\n  Edx Course on Sagemaker\n  "
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]