<!DOCTYPE html>
<html class="no-js" lang="en-US" prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#">
<html lang="en" class="js csstransforms3d">
  <head>
    <meta charset="utf-8">
    <meta property="og:title" content="RoboMakerWorkshops.com" />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://www.robomakerworkshops.com" />
    <meta property="og:image" content="https://www.robomakerworkshops.com/images/2_all_windows.png" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    <meta name="generator" content="Hugo 0.86.0" />
    <meta name="description" content="Amazon SageMaker Workshop">


    <link rel="shortcut icon" href="../../../images/favicon.ico" type="image/x-icon" />
<link rel="icon" href="../../../images/favicon.ico" type="image/x-icon" />

    <title> :: Get Started on Amazon SageMaker</title>

    
    <link href="../../../css/nucleus.css?1627670778" rel="stylesheet">
    <link href="../../../css/fontawesome-all.min.css?1627670778" rel="stylesheet">
    <link href="../../../css/hybrid.css?1627670778" rel="stylesheet">
    <link href="../../../css/featherlight.min.css?1627670778" rel="stylesheet">
    <link href="../../../css/perfect-scrollbar.min.css?1627670778" rel="stylesheet">
    <link href="../../../css/auto-complete.css?1627670778" rel="stylesheet">
    <link href="../../../css/theme.css?1627670778" rel="stylesheet">
    <link href="../../../css/hugo-theme.css?1627670778" rel="stylesheet">
    <link href="../../../css/jquery-ui.min.css?1627670778" rel="stylesheet">
    
      <link href="../../../css/theme-mine.css?1627670778" rel="stylesheet">
    

    <script src="../../../js/jquery-3.3.1.min.js?1627670778"></script>
    <script src="../../../js/jquery-ui-1.12.1.min.js?1627670778"></script>


    <style type="text/css">
      :root #header + #content > #left > #rlblock_left{
          display:none !important;
      }
      
        :not(pre) > code + span.copy-to-clipboard {
            display: none;
        }
      
    </style>
    
  </head>
  <body class="" data-url="../../../workshop/5_huggingface/hf_spot_notebook/">
    <nav id="sidebar" class="">



  <div id="header-wrapper">
    <div id="header">
      <a href="../../../" title="Go home"><svg id="Layer_1" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 30" width="60%" style="padding:20px 0px;"><defs><style>.cls-1{fill:#fff;}.cls-2{fill:#f90;fill-rule:evenodd;}</style></defs><title>AWS-Logo_White-Color</title><path class="cls-1" d="M14.09,10.85a4.7,4.7,0,0,0,.19,1.48,7.73,7.73,0,0,0,.54,1.19.77.77,0,0,1,.12.38.64.64,0,0,1-.32.49l-1,.7a.83.83,0,0,1-.44.15.69.69,0,0,1-.49-.23,3.8,3.8,0,0,1-.6-.77q-.25-.42-.51-1a6.14,6.14,0,0,1-4.89,2.3,4.54,4.54,0,0,1-3.32-1.19,4.27,4.27,0,0,1-1.22-3.2A4.28,4.28,0,0,1,3.61,7.75,6.06,6.06,0,0,1,7.69,6.46a12.47,12.47,0,0,1,1.76.13q.92.13,1.91.36V5.73a3.65,3.65,0,0,0-.79-2.66A3.81,3.81,0,0,0,7.86,2.3a7.71,7.71,0,0,0-1.79.22,12.78,12.78,0,0,0-1.79.57,4.55,4.55,0,0,1-.58.22l-.26,0q-.35,0-.35-.52V2a1.09,1.09,0,0,1,.12-.58,1.2,1.2,0,0,1,.47-.35A10.88,10.88,0,0,1,5.77.32,10.19,10.19,0,0,1,8.36,0a6,6,0,0,1,4.35,1.35,5.49,5.49,0,0,1,1.38,4.09ZM7.34,13.38a5.36,5.36,0,0,0,1.72-.31A3.63,3.63,0,0,0,10.63,12,2.62,2.62,0,0,0,11.19,11a5.63,5.63,0,0,0,.16-1.44v-.7a14.35,14.35,0,0,0-1.53-.28,12.37,12.37,0,0,0-1.56-.1,3.84,3.84,0,0,0-2.47.67A2.34,2.34,0,0,0,5,11a2.35,2.35,0,0,0,.61,1.76A2.4,2.4,0,0,0,7.34,13.38Zm13.35,1.8a1,1,0,0,1-.64-.16,1.3,1.3,0,0,1-.35-.65L15.81,1.51a3,3,0,0,1-.15-.67.36.36,0,0,1,.41-.41H17.7a1,1,0,0,1,.65.16,1.4,1.4,0,0,1,.33.65l2.79,11,2.59-11A1.17,1.17,0,0,1,24.39.6a1.1,1.1,0,0,1,.67-.16H26.4a1.1,1.1,0,0,1,.67.16,1.17,1.17,0,0,1,.32.65L30,12.39,32.88,1.25A1.39,1.39,0,0,1,33.22.6a1,1,0,0,1,.65-.16h1.54a.36.36,0,0,1,.41.41,1.36,1.36,0,0,1,0,.26,3.64,3.64,0,0,1-.12.41l-4,12.86a1.3,1.3,0,0,1-.35.65,1,1,0,0,1-.64.16H29.25a1,1,0,0,1-.67-.17,1.26,1.26,0,0,1-.32-.67L25.67,3.64,23.11,14.34a1.26,1.26,0,0,1-.32.67,1,1,0,0,1-.67.17Zm21.36.44a11.28,11.28,0,0,1-2.56-.29,7.44,7.44,0,0,1-1.92-.67,1,1,0,0,1-.61-.93v-.84q0-.52.38-.52a.9.9,0,0,1,.31.06l.42.17a8.77,8.77,0,0,0,1.83.58,9.78,9.78,0,0,0,2,.2,4.48,4.48,0,0,0,2.43-.55,1.76,1.76,0,0,0,.86-1.57,1.61,1.61,0,0,0-.45-1.16A4.29,4.29,0,0,0,43,9.22l-2.41-.76A5.15,5.15,0,0,1,38,6.78a3.94,3.94,0,0,1-.83-2.41,3.7,3.7,0,0,1,.45-1.85,4.47,4.47,0,0,1,1.19-1.37A5.27,5.27,0,0,1,40.51.29,7.4,7.4,0,0,1,42.6,0a8.87,8.87,0,0,1,1.12.07q.57.07,1.08.19t.95.26a4.27,4.27,0,0,1,.7.29,1.59,1.59,0,0,1,.49.41.94.94,0,0,1,.15.55v.79q0,.52-.38.52a1.76,1.76,0,0,1-.64-.2,7.74,7.74,0,0,0-3.2-.64,4.37,4.37,0,0,0-2.21.47,1.6,1.6,0,0,0-.79,1.48,1.58,1.58,0,0,0,.49,1.18,4.94,4.94,0,0,0,1.83.92L44.55,7a5.08,5.08,0,0,1,2.57,1.6A3.76,3.76,0,0,1,47.9,11a4.21,4.21,0,0,1-.44,1.93,4.4,4.4,0,0,1-1.21,1.47,5.43,5.43,0,0,1-1.85.93A8.25,8.25,0,0,1,42.05,15.62Z"></path><path class="cls-2" d="M45.19,23.81C39.72,27.85,31.78,30,25,30A36.64,36.64,0,0,1,.22,20.57c-.51-.46-.06-1.09.56-.74A49.78,49.78,0,0,0,25.53,26.4,49.23,49.23,0,0,0,44.4,22.53C45.32,22.14,46.1,23.14,45.19,23.81Z"></path><path class="cls-2" d="M47.47,21.21c-.7-.9-4.63-.42-6.39-.21-.53.06-.62-.4-.14-.74,3.13-2.2,8.27-1.57,8.86-.83s-.16,5.89-3.09,8.35c-.45.38-.88.18-.68-.32C46.69,25.8,48.17,22.11,47.47,21.21Z"></path></svg></a>

    </div>
    
        <div class="searchbox">
    <label for="search-by"><i class="fas fa-search"></i></label>
    <input data-search-input id="search-by" type="text" placeholder="Search...">
    <span data-search-clear=""><i class="fas fa-close"></i></span>
</div>

<script type="text/javascript" src="../../../js/lunr.min.js?1627670778"></script>
<script type="text/javascript" src="../../../js/auto-complete.js?1627670778"></script>
<script type="text/javascript">
    
        var baseurl = '\/';
    
</script>
<script type="text/javascript" src="../../../js/search.js?1627670778"></script>

    
  </div>

    <div class="highlightable">
    <ul class="topics">

        
          
          


 
  
    
    <li data-nav-id="/workshop/" title="NLP on AWS" class="dd-item 
        parent
        
        
        ">
      <a href="../../../workshop/">
          NLP on AWS
          
      </a>
      
      
        <ul>
          
          
            
          
          
          
        
          
            
            


 
  
    
    <li data-nav-id="/workshop/1_gettingstarted/" title="Getting Started" class="dd-item 
        
        
        
        ">
      <a href="../../../workshop/1_gettingstarted/">
          Getting Started
          
      </a>
      
      
        <ul>
          
          
          
          
        
          
            
            


 
  
    
      <li data-nav-id="/workshop/1_gettingstarted/create/" title="Setup" class="dd-item ">
        <a href="../../../workshop/1_gettingstarted/create/">
        Setup
        
        </a>
    </li>
     
  
 

            
          
            
            


 
  
    
      <li data-nav-id="/workshop/1_gettingstarted/clone/" title="Clone the Repository" class="dd-item ">
        <a href="../../../workshop/1_gettingstarted/clone/">
        Clone the Repository
        
        </a>
    </li>
     
  
 

            
          
        
        </ul>
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/workshop/2_builtin/" title="Built-in Algorithms" class="dd-item 
        
        
        
        ">
      <a href="../../../workshop/2_builtin/">
          Built-in Algorithms
          
      </a>
      
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/workshop/3_scriptmode/" title="SageMaker Script Mode" class="dd-item 
        
        
        
        ">
      <a href="../../../workshop/3_scriptmode/">
          SageMaker Script Mode
          
      </a>
      
      
        <ul>
          
          
          
          
        
          
            
            


 
  
    
      <li data-nav-id="/workshop/3_scriptmode/pytorch_rnn/" title="Word-level language modeling using PyTorch" class="dd-item ">
        <a href="../../../workshop/3_scriptmode/pytorch_rnn/">
        Word-level language modeling using PyTorch
        
        </a>
    </li>
     
  
 

            
          
        
        </ul>
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/workshop/4_byoc/" title="Hugging Face" class="dd-item 
        
        
        
        ">
      <a href="../../../workshop/4_byoc/">
          Hugging Face
          
      </a>
      
      
        <ul>
          
          
          
          
        
          
            
            


 
  
    
      <li data-nav-id="/workshop/4_byoc/pytorch_rnn/" title="Word-level language modeling using PyTorch" class="dd-item ">
        <a href="../../../workshop/4_byoc/pytorch_rnn/">
        Word-level language modeling using PyTorch
        
        </a>
    </li>
     
  
 

            
          
        
        </ul>
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/workshop/5_huggingface/" title="Use a custom Docker image" class="dd-item 
        parent
        
        
        ">
      <a href="../../../workshop/5_huggingface/">
          Use a custom Docker image
          
      </a>
      
      
        <ul>
          
          
          
          
        
          
            
            


 
  
    
      <li data-nav-id="/workshop/5_huggingface/hf_spot_notebook/" title="" class="dd-item active">
        <a href="../../../workshop/5_huggingface/hf_spot_notebook/">
        
        
        </a>
    </li>
     
  
 

            
          
        
        </ul>
              
    </li>
  
 

            
          
        
        </ul>
              
    </li>
  
 

          
         
    </ul>

    
    
      <section id="shortcuts">
        <h3>More</h3>
        <ul>
          
              <li> 
                  <a class="padding" href="../../../more_resources"><i class='fas fa-bookmark'></i> More Resources</a>
              </li>
          
              <li> 
                  <a class="padding" href="../../../authors"><i class='fas fa-users'></i> Authors</a>
              </li>
          
        </ul>
      </section>
    

    
    <section id="footer">
      <left>

    <h2 class="github-title">Amazon SageMaker</h2>
    <h5 class="copyright">&copy; 2020 Amazon Web Services, Inc. or its Affiliates. All rights reserved.<h5>

</left>

<script async defer src="https://buttons.github.io/buttons.js"></script>

    </section>
  </div>
</nav>





        <section id="body">
        <div id="overlay"></div>
        <div class="padding highlightable">
              
              <div>
                <div id="top-bar">
                
                  
                  
                  
                  <div id="top-github-link">
                    <a class="github-link" title='Edit this page' href="https://github.com/w601sxs/aws-sagemaker-workshop/edit/master/content/workshop/5_HuggingFace/HF_spot_notebook.md" target="blank">
                      <i class="fas fa-code-branch"></i>
                      <span id="top-github-link-text">Edit this page</span>
                    </a>
                  </div>
                  
                
                
                <div id="breadcrumbs" itemscope="" itemtype="http://data-vocabulary.org/Breadcrumb">
                    <span id="sidebar-toggle-span">
                        <a href="#" id="sidebar-toggle" data-sidebar-toggle="">
                          <i class="fas fa-bars"></i>
                        </a>
                    </span>
                  
                  <span id="toc-menu"><i class="fas fa-list-alt"></i></span>
                  
                  <span class="links">
                    
          
          
            
            
          
          
            
            
          
          
            
            
          
          
            <a href='../../../'>NLP Workshop</a> > <a href='../../../workshop/'>NLP on AWS</a> > <a href='../../../workshop/5_huggingface/'>Use a custom Docker image</a> > 
          
         
          
         
          
         
          
           
                  </span>
                </div>
                
                    <div class="progress">
    <div class="wrapper">
<nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#binary-classification-with-trainer-and-imdb-dataset">Binary Classification with <code>Trainer</code> and <code>imdb</code> dataset</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#installation">Installation</a></li>
    <li><a href="#development-environment">Development environment</a></li>
    <li><a href="#permissions">Permissions</a></li>
  </ul>

  <ul>
    <li><a href="#tokenization">Tokenization</a></li>
    <li><a href="#uploading-data-to-sagemaker_session_bucket">Uploading data to <code>sagemaker_session_bucket</code></a></li>
  </ul>

  <ul>
    <li><a href="#creating-an-estimator-and-start-a-training-job">Creating an Estimator and start a training job</a></li>
    <li><a href="#deploying-the-endpoint">Deploying the endpoint</a></li>
  </ul>

  <ul>
    <li><a href="#estimator-parameters">Estimator Parameters</a></li>
    <li><a href="#attach-to-old-training-job-to-an-estimator">Attach to old training job to an estimator</a></li>
  </ul>
</nav>
    </div>
</div>

                
              </div>
            </div>
            

        
        <div id="body-inner">
          
            
          

        


<h1></h1>

<h1 id="huggingface-sagemaker-sdk---spot-instances-example">Huggingface Sagemaker-sdk - Spot instances example</h1>
<h3 id="binary-classification-with-trainer-and-imdb-dataset">Binary Classification with <code>Trainer</code> and <code>imdb</code> dataset</h3>
<ol>
<li><a href="#Introduction">Introduction</a></li>
<li><a href="#Development-Environment-and-Permissions">Development Environment and Permissions</a>
<ol>
<li><a href="#Installation">Installation</a></li>
<li><a href="#Development-environment">Development environment</a></li>
<li><a href="#Permissions">Permissions</a></li>
</ol>
</li>
<li><a href="#Preprocessing">Processing</a>
<ol>
<li><a href="#Tokenization">Tokenization</a></li>
<li><a href="#Uploading-data-to-sagemaker_session_bucket">Uploading data to sagemaker_session_bucket</a></li>
</ol>
</li>
<li><a href="#Fine-tuning-&amp;-starting-Sagemaker-Training-Job">Fine-tuning &amp; starting Sagemaker Training Job</a>
<ol>
<li><a href="#Creating-an-Estimator-and-start-a-training-job">Creating an Estimator and start a training job</a></li>
<li><a href="#Estimator-Parameters">Estimator Parameters</a></li>
<li><a href="#Download-fine-tuned-model-from-s3">Download fine-tuned model from s3</a></li>
<li><a href="#Attach-to-old-training-job-to-an-estimator">Attach to old training job to an estimator </a></li>
</ol>
</li>
<li><a href="#Push-model-to-the-Hugging-Face-hub"><em>Coming soon</em>:Push model to the Hugging Face hub</a></li>
</ol>
<h1 id="introduction">Introduction</h1>
<p>Welcome to our end-to-end binary Text-Classification example. In this demo, we will use the Hugging Faces <code>transformers</code> and <code>datasets</code> library together with a custom Amazon sagemaker-sdk extension to fine-tune a pre-trained transformer on binary text classification. In particular, the pre-trained model will be fine-tuned using the <code>imdb</code> dataset. To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on. This demo will also show you can use spot instances and continue training.</p>
<p><img src="attachment:image.png" alt="image.png"></p>
<p><em><strong>NOTE: You can run this demo in Sagemaker Studio, your local machine or Sagemaker Notebook Instances</strong></em></p>
<h1 id="development-environment-and-permissions">Development Environment and Permissions</h1>
<h2 id="installation">Installation</h2>
<p><em><em>Note:</em> we only install the required libraries from Hugging Face and AWS. You also need PyTorch or Tensorflow, if you haven´t it installed</em></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#960050;background-color:#1e0010">!</span>pip install <span style="color:#e6db74">&#34;sagemaker&gt;=2.48.0&#34;</span> <span style="color:#e6db74">&#34;transformers==4.6.1&#34;</span> <span style="color:#e6db74">&#34;datasets[s3]==1.6.2&#34;</span> <span style="color:#f92672">--</span>upgrade
</code></pre></div><h2 id="development-environment">Development environment</h2>
<p><strong>upgrade ipywidgets for <code>datasets</code> library and restart kernel, only needed when prerpocessing is done in the notebook</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">%%</span>capture
<span style="color:#f92672">import</span> IPython
<span style="color:#960050;background-color:#1e0010">!</span>conda install <span style="color:#f92672">-</span>c conda<span style="color:#f92672">-</span>forge ipywidgets <span style="color:#f92672">-</span>y
IPython<span style="color:#f92672">.</span>Application<span style="color:#f92672">.</span>instance()<span style="color:#f92672">.</span>kernel<span style="color:#f92672">.</span>do_shutdown(<span style="color:#66d9ef">True</span>) <span style="color:#75715e"># has to restart kernel so changes are used</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> sagemaker.huggingface
</code></pre></div><h2 id="permissions">Permissions</h2>
<p><em>If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html">here</a> more about it.</em></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> sagemaker

sess <span style="color:#f92672">=</span> sagemaker<span style="color:#f92672">.</span>Session()
<span style="color:#75715e"># sagemaker session bucket -&gt; used for uploading data, models and logs</span>
<span style="color:#75715e"># sagemaker will automatically create this bucket if it not exists</span>
sagemaker_session_bucket<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>
<span style="color:#66d9ef">if</span> sagemaker_session_bucket <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span> <span style="color:#f92672">and</span> sess <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
    <span style="color:#75715e"># set to default bucket if a bucket name is not given</span>
    sagemaker_session_bucket <span style="color:#f92672">=</span> sess<span style="color:#f92672">.</span>default_bucket()

role <span style="color:#f92672">=</span> sagemaker<span style="color:#f92672">.</span>get_execution_role()
sess <span style="color:#f92672">=</span> sagemaker<span style="color:#f92672">.</span>Session(default_bucket<span style="color:#f92672">=</span>sagemaker_session_bucket)

print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;sagemaker role arn: </span><span style="color:#e6db74">{</span>role<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;sagemaker bucket: </span><span style="color:#e6db74">{</span>sess<span style="color:#f92672">.</span>default_bucket()<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;sagemaker session region: </span><span style="color:#e6db74">{</span>sess<span style="color:#f92672">.</span>boto_region_name<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</code></pre></div><h1 id="preprocessing">Preprocessing</h1>
<p>We are using the <code>datasets</code> library to download and preprocess the <code>imdb</code> dataset. After preprocessing, the dataset will be uploaded to our <code>sagemaker_session_bucket</code> to be used within our training job. The <a href="http://ai.stanford.edu/~amaas/data/sentiment/">imdb</a> dataset consists of 25000 training and 25000 testing highly polar movie reviews.</p>
<h2 id="tokenization">Tokenization</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> datasets <span style="color:#f92672">import</span> load_dataset
<span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoTokenizer

<span style="color:#75715e"># tokenizer used in preprocessing</span>
tokenizer_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;distilbert-base-uncased&#39;</span>

<span style="color:#75715e"># dataset used</span>
dataset_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;imdb&#39;</span>

<span style="color:#75715e"># s3 key prefix for the data</span>
s3_prefix <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;samples/datasets/imdb&#39;</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># load dataset</span>
dataset <span style="color:#f92672">=</span> load_dataset(dataset_name)

<span style="color:#75715e"># download tokenizer</span>
tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(tokenizer_name)

<span style="color:#75715e"># tokenizer helper function</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">tokenize</span>(batch):
    <span style="color:#66d9ef">return</span> tokenizer(batch[<span style="color:#e6db74">&#39;text&#39;</span>], padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;max_length&#39;</span>, truncation<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)

<span style="color:#75715e"># load dataset</span>
train_dataset, test_dataset <span style="color:#f92672">=</span> load_dataset(<span style="color:#e6db74">&#39;imdb&#39;</span>, split<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;train&#39;</span>, <span style="color:#e6db74">&#39;test&#39;</span>])
test_dataset <span style="color:#f92672">=</span> test_dataset<span style="color:#f92672">.</span>shuffle()<span style="color:#f92672">.</span>select(range(<span style="color:#ae81ff">10000</span>)) <span style="color:#75715e"># smaller the size for test dataset to 10k </span>


<span style="color:#75715e"># tokenize dataset</span>
train_dataset <span style="color:#f92672">=</span> train_dataset<span style="color:#f92672">.</span>map(tokenize, batched<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
test_dataset <span style="color:#f92672">=</span> test_dataset<span style="color:#f92672">.</span>map(tokenize, batched<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)

<span style="color:#75715e"># set format for pytorch</span>
train_dataset <span style="color:#f92672">=</span>  train_dataset<span style="color:#f92672">.</span>rename_column(<span style="color:#e6db74">&#34;label&#34;</span>, <span style="color:#e6db74">&#34;labels&#34;</span>)
train_dataset<span style="color:#f92672">.</span>set_format(<span style="color:#e6db74">&#39;torch&#39;</span>, columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;input_ids&#39;</span>, <span style="color:#e6db74">&#39;attention_mask&#39;</span>, <span style="color:#e6db74">&#39;labels&#39;</span>])
test_dataset <span style="color:#f92672">=</span> test_dataset<span style="color:#f92672">.</span>rename_column(<span style="color:#e6db74">&#34;label&#34;</span>, <span style="color:#e6db74">&#34;labels&#34;</span>)
test_dataset<span style="color:#f92672">.</span>set_format(<span style="color:#e6db74">&#39;torch&#39;</span>, columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;input_ids&#39;</span>, <span style="color:#e6db74">&#39;attention_mask&#39;</span>, <span style="color:#e6db74">&#39;labels&#39;</span>])
</code></pre></div><h2 id="uploading-data-to-sagemaker_session_bucket">Uploading data to <code>sagemaker_session_bucket</code></h2>
<p>After we processed the <code>datasets</code> we are going to use the new <code>FileSystem</code> <a href="https://huggingface.co/docs/datasets/filesystems.html">integration</a> to upload our dataset to S3.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> botocore
<span style="color:#f92672">from</span> datasets.filesystems <span style="color:#f92672">import</span> S3FileSystem

s3 <span style="color:#f92672">=</span> S3FileSystem()  

<span style="color:#75715e"># save train_dataset to s3</span>
training_input_path <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;s3://</span><span style="color:#e6db74">{</span>sess<span style="color:#f92672">.</span>default_bucket()<span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">{</span>s3_prefix<span style="color:#e6db74">}</span><span style="color:#e6db74">/train&#39;</span>
train_dataset<span style="color:#f92672">.</span>save_to_disk(training_input_path,fs<span style="color:#f92672">=</span>s3)

<span style="color:#75715e"># save test_dataset to s3</span>
test_input_path <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;s3://</span><span style="color:#e6db74">{</span>sess<span style="color:#f92672">.</span>default_bucket()<span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">{</span>s3_prefix<span style="color:#e6db74">}</span><span style="color:#e6db74">/test&#39;</span>
test_dataset<span style="color:#f92672">.</span>save_to_disk(test_input_path,fs<span style="color:#f92672">=</span>s3)

</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">training_input_path <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;s3://</span><span style="color:#e6db74">{</span>sess<span style="color:#f92672">.</span>default_bucket()<span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">{</span>s3_prefix<span style="color:#e6db74">}</span><span style="color:#e6db74">/train&#39;</span>
test_input_path <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;s3://</span><span style="color:#e6db74">{</span>sess<span style="color:#f92672">.</span>default_bucket()<span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">{</span>s3_prefix<span style="color:#e6db74">}</span><span style="color:#e6db74">/test&#39;</span>

</code></pre></div><h1 id="fine-tuning--starting-sagemaker-training-job">Fine-tuning &amp; starting Sagemaker Training Job</h1>
<p>In order to create a sagemaker training job we need an <code>HuggingFace</code> Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as <code>entry_point</code>, which <code>instance_type</code> should be used, which <code>hyperparameters</code> are passed in &hellip;..</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">huggingface_estimator <span style="color:#f92672">=</span> HuggingFace(entry_point<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;train.py&#39;</span>,
                            source_dir<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;./scripts&#39;</span>,
                            base_job_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;huggingface-sdk-extension&#39;</span>,
                            instance_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;ml.p3.2xlarge&#39;</span>,
                            instance_count<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,
                            transformers_version<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;4.4&#39;</span>,
                            pytorch_version<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;1.6&#39;</span>,
                            py_version<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;py36&#39;</span>,
                            role<span style="color:#f92672">=</span>role,
                            hyperparameters <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;epochs&#39;</span>: <span style="color:#ae81ff">1</span>,
                                               <span style="color:#e6db74">&#39;train_batch_size&#39;</span>: <span style="color:#ae81ff">32</span>,
                                               <span style="color:#e6db74">&#39;model_name&#39;</span>:<span style="color:#e6db74">&#39;distilbert-base-uncased&#39;</span>
                                                })
</code></pre></div><p>When we create a SageMaker training job, SageMaker takes care of starting and managing all the required ec2 instances for us with the <code>huggingface</code> container, uploads the provided fine-tuning script <code>train.py</code> and downloads the data from our <code>sagemaker_session_bucket</code> into the container at <code>/opt/ml/input/data</code>. Then, it starts the training job by running.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">/</span>opt<span style="color:#f92672">/</span>conda<span style="color:#f92672">/</span>bin<span style="color:#f92672">/</span>python train<span style="color:#f92672">.</span>py <span style="color:#f92672">--</span>epochs <span style="color:#ae81ff">1</span> <span style="color:#f92672">--</span>model_name distilbert<span style="color:#f92672">-</span>base<span style="color:#f92672">-</span>uncased <span style="color:#f92672">--</span>train_batch_size <span style="color:#ae81ff">32</span>
</code></pre></div><p>The <code>hyperparameters</code> you define in the <code>HuggingFace</code> estimator are passed in as named arguments.</p>
<p>Sagemaker is providing useful properties about the training environment through various environment variables, including the following:</p>
<ul>
<li>
<p><code>SM_MODEL_DIR</code>: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting.</p>
</li>
<li>
<p><code>SM_NUM_GPUS</code>: An integer representing the number of GPUs available to the host.</p>
</li>
<li>
<p><code>SM_CHANNEL_XXXX:</code> A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimator’s fit call, named <code>train</code> and <code>test</code>, the environment variables <code>SM_CHANNEL_TRAIN</code> and <code>SM_CHANNEL_TEST</code> are set.</p>
</li>
</ul>
<p>To run your training job locally you can define <code>instance_type='local'</code> or <code>instance_type='local-gpu'</code> for gpu usage. <em>Note: this does not working within SageMaker Studio</em></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#960050;background-color:#1e0010">!</span>pygmentize <span style="color:#f92672">./</span>scripts<span style="color:#f92672">/</span>train<span style="color:#f92672">.</span>py
</code></pre></div><pre><code>[34mfrom[39;49;00m [04m[36mtransformers[39;49;00m [34mimport[39;49;00m AutoModelForSequenceClassification, Trainer, TrainingArguments
[34mfrom[39;49;00m [04m[36mtransformers[39;49;00m[04m[36m.[39;49;00m[04m[36mtrainer_utils[39;49;00m [34mimport[39;49;00m get_last_checkpoint

[34mfrom[39;49;00m [04m[36msklearn[39;49;00m[04m[36m.[39;49;00m[04m[36mmetrics[39;49;00m [34mimport[39;49;00m accuracy_score, precision_recall_fscore_support
[34mfrom[39;49;00m [04m[36mdatasets[39;49;00m [34mimport[39;49;00m load_from_disk
[34mimport[39;49;00m [04m[36mlogging[39;49;00m
[34mimport[39;49;00m [04m[36msys[39;49;00m
[34mimport[39;49;00m [04m[36margparse[39;49;00m
[34mimport[39;49;00m [04m[36mos[39;49;00m

[37m# Set up logging[39;49;00m
logger = logging.getLogger([31m__name__[39;49;00m)

logging.basicConfig(
    level=logging.getLevelName([33m&quot;[39;49;00m[33mINFO[39;49;00m[33m&quot;[39;49;00m),
    handlers=[logging.StreamHandler(sys.stdout)],
    [36mformat[39;49;00m=[33m&quot;[39;49;00m[33m%(asctime)s[39;49;00m[33m - [39;49;00m[33m%(name)s[39;49;00m[33m - [39;49;00m[33m%(levelname)s[39;49;00m[33m - [39;49;00m[33m%(message)s[39;49;00m[33m&quot;[39;49;00m,
)

[34mif[39;49;00m [31m__name__[39;49;00m == [33m&quot;[39;49;00m[33m__main__[39;49;00m[33m&quot;[39;49;00m:

    logger.info(sys.argv)

    parser = argparse.ArgumentParser()

    [37m# hyperparameters sent by the client are passed as command-line arguments to the script.[39;49;00m
    parser.add_argument([33m&quot;[39;49;00m[33m--epochs[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mint[39;49;00m, default=[34m3[39;49;00m)
    parser.add_argument([33m&quot;[39;49;00m[33m--train-batch-size[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mint[39;49;00m, default=[34m32[39;49;00m)
    parser.add_argument([33m&quot;[39;49;00m[33m--eval-batch-size[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mint[39;49;00m, default=[34m64[39;49;00m)
    parser.add_argument([33m&quot;[39;49;00m[33m--warmup_steps[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mint[39;49;00m, default=[34m500[39;49;00m)
    parser.add_argument([33m&quot;[39;49;00m[33m--model_name[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mstr[39;49;00m)
    parser.add_argument([33m&quot;[39;49;00m[33m--learning_rate[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mstr[39;49;00m, default=[34m5e-5[39;49;00m)
    parser.add_argument([33m&quot;[39;49;00m[33m--output_dir[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mstr[39;49;00m)

    [37m# Data, model, and output directories[39;49;00m
    parser.add_argument([33m&quot;[39;49;00m[33m--output-data-dir[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mstr[39;49;00m, default=os.environ[[33m&quot;[39;49;00m[33mSM_OUTPUT_DATA_DIR[39;49;00m[33m&quot;[39;49;00m])
    parser.add_argument([33m&quot;[39;49;00m[33m--model-dir[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mstr[39;49;00m, default=os.environ[[33m&quot;[39;49;00m[33mSM_MODEL_DIR[39;49;00m[33m&quot;[39;49;00m])
    parser.add_argument([33m&quot;[39;49;00m[33m--n_gpus[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mstr[39;49;00m, default=os.environ[[33m&quot;[39;49;00m[33mSM_NUM_GPUS[39;49;00m[33m&quot;[39;49;00m])
    parser.add_argument([33m&quot;[39;49;00m[33m--training_dir[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mstr[39;49;00m, default=os.environ[[33m&quot;[39;49;00m[33mSM_CHANNEL_TRAIN[39;49;00m[33m&quot;[39;49;00m])
    parser.add_argument([33m&quot;[39;49;00m[33m--test_dir[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mstr[39;49;00m, default=os.environ[[33m&quot;[39;49;00m[33mSM_CHANNEL_TEST[39;49;00m[33m&quot;[39;49;00m])

    args, _ = parser.parse_known_args()

    [37m# load datasets[39;49;00m
    train_dataset = load_from_disk(args.training_dir)
    test_dataset = load_from_disk(args.test_dir)

    logger.info([33mf[39;49;00m[33m&quot;[39;49;00m[33m loaded train_dataset length is: [39;49;00m[33m{[39;49;00m[36mlen[39;49;00m(train_dataset)[33m}[39;49;00m[33m&quot;[39;49;00m)
    logger.info([33mf[39;49;00m[33m&quot;[39;49;00m[33m loaded test_dataset length is: [39;49;00m[33m{[39;49;00m[36mlen[39;49;00m(test_dataset)[33m}[39;49;00m[33m&quot;[39;49;00m)

    [37m# compute metrics function for binary classification[39;49;00m
    [34mdef[39;49;00m [32mcompute_metrics[39;49;00m(pred):
        labels = pred.label_ids
        preds = pred.predictions.argmax(-[34m1[39;49;00m)
        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=[33m&quot;[39;49;00m[33mbinary[39;49;00m[33m&quot;[39;49;00m)
        acc = accuracy_score(labels, preds)
        [34mreturn[39;49;00m {[33m&quot;[39;49;00m[33maccuracy[39;49;00m[33m&quot;[39;49;00m: acc, [33m&quot;[39;49;00m[33mf1[39;49;00m[33m&quot;[39;49;00m: f1, [33m&quot;[39;49;00m[33mprecision[39;49;00m[33m&quot;[39;49;00m: precision, [33m&quot;[39;49;00m[33mrecall[39;49;00m[33m&quot;[39;49;00m: recall}

    [37m# download model from model hub[39;49;00m
    model = AutoModelForSequenceClassification.from_pretrained(args.model_name)

    [37m# define training args[39;49;00m
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.train_batch_size,
        per_device_eval_batch_size=args.eval_batch_size,
        warmup_steps=args.warmup_steps,
        evaluation_strategy=[33m&quot;[39;49;00m[33mepoch[39;49;00m[33m&quot;[39;49;00m,
        logging_dir=[33mf[39;49;00m[33m&quot;[39;49;00m[33m{[39;49;00margs.output_data_dir[33m}[39;49;00m[33m/logs[39;49;00m[33m&quot;[39;49;00m,
        learning_rate=[36mfloat[39;49;00m(args.learning_rate),
    )

    [37m# create Trainer instance[39;49;00m
    trainer = Trainer(
        model=model,
        args=training_args,
        compute_metrics=compute_metrics,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
    )

    [37m# train model[39;49;00m
    [34mif[39;49;00m get_last_checkpoint(args.output_dir) [35mis[39;49;00m [35mnot[39;49;00m [34mNone[39;49;00m:
        logger.info([33m&quot;[39;49;00m[33m***** continue training *****[39;49;00m[33m&quot;[39;49;00m)
        trainer.train(resume_from_checkpoint=args.output_dir)
    [34melse[39;49;00m:
        trainer.train()
    [37m# evaluate model[39;49;00m
    eval_result = trainer.evaluate(eval_dataset=test_dataset)

    [37m# writes eval result to file which can be accessed later in s3 ouput[39;49;00m
    [34mwith[39;49;00m [36mopen[39;49;00m(os.path.join(args.output_data_dir, [33m&quot;[39;49;00m[33meval_results.txt[39;49;00m[33m&quot;[39;49;00m), [33m&quot;[39;49;00m[33mw[39;49;00m[33m&quot;[39;49;00m) [34mas[39;49;00m writer:
        [36mprint[39;49;00m([33mf[39;49;00m[33m&quot;[39;49;00m[33m***** Eval results *****[39;49;00m[33m&quot;[39;49;00m)
        [34mfor[39;49;00m key, value [35min[39;49;00m [36msorted[39;49;00m(eval_result.items()):
            writer.write([33mf[39;49;00m[33m&quot;[39;49;00m[33m{[39;49;00mkey[33m}[39;49;00m[33m = [39;49;00m[33m{[39;49;00mvalue[33m}[39;49;00m[33m\n[39;49;00m[33m&quot;[39;49;00m)

    [37m# Saves the model to s3[39;49;00m
    trainer.save_model(args.model_dir)
</code></pre>
<h2 id="creating-an-estimator-and-start-a-training-job">Creating an Estimator and start a training job</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sagemaker.huggingface <span style="color:#f92672">import</span> HuggingFace

<span style="color:#75715e"># hyperparameters, which are passed into the training job</span>
hyperparameters<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#39;epochs&#39;</span>: <span style="color:#ae81ff">1</span>,
                 <span style="color:#e6db74">&#39;train_batch_size&#39;</span>: <span style="color:#ae81ff">32</span>,
                 <span style="color:#e6db74">&#39;model_name&#39;</span>:<span style="color:#e6db74">&#39;distilbert-base-uncased&#39;</span>,
                 <span style="color:#e6db74">&#39;output_dir&#39;</span>:<span style="color:#e6db74">&#39;/opt/ml/checkpoints&#39;</span>
                 }

<span style="color:#75715e"># s3 uri where our checkpoints will be uploaded during training</span>
job_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;using-spot&#34;</span>
checkpoint_s3_uri <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;s3://</span><span style="color:#e6db74">{</span>sess<span style="color:#f92672">.</span>default_bucket()<span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">{</span>job_name<span style="color:#e6db74">}</span><span style="color:#e6db74">/checkpoints&#39;</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">huggingface_estimator <span style="color:#f92672">=</span> HuggingFace(entry_point<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;train.py&#39;</span>,
                            source_dir<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;./scripts&#39;</span>,
                            instance_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;ml.p3.2xlarge&#39;</span>,
                            instance_count<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,
                            base_job_name<span style="color:#f92672">=</span>job_name,
                            checkpoint_s3_uri<span style="color:#f92672">=</span>checkpoint_s3_uri,
                            use_spot_instances<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
                            max_wait<span style="color:#f92672">=</span><span style="color:#ae81ff">3600</span>, <span style="color:#75715e"># This should be equal to or greater than max_run in seconds&#39;</span>
                            max_run<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, <span style="color:#75715e"># expected max run in seconds</span>
                            role<span style="color:#f92672">=</span>role,
                            transformers_version<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;4.6&#39;</span>,
                            pytorch_version<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;1.7&#39;</span>,
                            py_version<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;py36&#39;</span>,
                            hyperparameters <span style="color:#f92672">=</span> hyperparameters)
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># starting the train job with our uploaded datasets as input</span>
huggingface_estimator<span style="color:#f92672">.</span>fit({<span style="color:#e6db74">&#39;train&#39;</span>: training_input_path, <span style="color:#e6db74">&#39;test&#39;</span>: test_input_path})

<span style="color:#75715e"># Training seconds: 874</span>
<span style="color:#75715e"># Billable seconds: 262</span>
<span style="color:#75715e"># Managed Spot Training savings: 70.0%</span>
</code></pre></div><h2 id="deploying-the-endpoint">Deploying the endpoint</h2>
<p>To deploy our endpoint, we call <code>deploy()</code> on our HuggingFace estimator object, passing in our desired number of instances and instance type.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">predictor <span style="color:#f92672">=</span> huggingface_estimator<span style="color:#f92672">.</span>deploy(<span style="color:#ae81ff">1</span>,<span style="color:#e6db74">&#34;ml.g4dn.xlarge&#34;</span>)
</code></pre></div><p>Then, we use the returned predictor object to call the endpoint.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">sentiment_input<span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;inputs&#34;</span>:<span style="color:#e6db74">&#34;I love using the new Inference DLC.&#34;</span>}

predictor<span style="color:#f92672">.</span>predict(sentiment_input)
</code></pre></div><p>Finally, we delete the endpoint again.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">predictor<span style="color:#f92672">.</span>delete_endpoint()
</code></pre></div><h1 id="extras">Extras</h1>
<h2 id="estimator-parameters">Estimator Parameters</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># container image used for training job</span>
print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;container image used for training job: </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">{</span>huggingface_estimator<span style="color:#f92672">.</span>image_uri<span style="color:#e6db74">}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)

<span style="color:#75715e"># s3 uri where the trained model is located</span>
print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;s3 uri where the trained model is located: </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">{</span>huggingface_estimator<span style="color:#f92672">.</span>model_data<span style="color:#e6db74">}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)

<span style="color:#75715e"># latest training job name for this estimator</span>
print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;latest training job name for this estimator: </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">{</span>huggingface_estimator<span style="color:#f92672">.</span>latest_training_job<span style="color:#f92672">.</span>name<span style="color:#e6db74">}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)


</code></pre></div><pre><code>container image used for training job: 
558105141721.dkr.ecr.us-east-1.amazonaws.com/huggingface-training:pytorch1.6.0-transformers4.2.2-tokenizers0.9.4-datasets1.2.1-py36-gpu-cu110

s3 uri where the trained model is located: 
s3://philipps-sagemaker-bucket-us-east-1/huggingface-training-2021-02-04-16-47-39-189/output/model.tar.gz

latest training job name for this estimator: 
huggingface-training-2021-02-04-16-47-39-189
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># access the logs of the training job</span>
huggingface_estimator<span style="color:#f92672">.</span>sagemaker_session<span style="color:#f92672">.</span>logs_for_job(huggingface_estimator<span style="color:#f92672">.</span>latest_training_job<span style="color:#f92672">.</span>name)
</code></pre></div><h2 id="attach-to-old-training-job-to-an-estimator">Attach to old training job to an estimator</h2>
<p>In Sagemaker you can attach an old training job to an estimator to continue training, get results etc..</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sagemaker.estimator <span style="color:#f92672">import</span> Estimator

<span style="color:#75715e"># job which is going to be attached to the estimator</span>
old_training_job_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;&#39;</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># attach old training job</span>
huggingface_estimator_loaded <span style="color:#f92672">=</span> Estimator<span style="color:#f92672">.</span>attach(old_training_job_name)

<span style="color:#75715e"># get model output s3 from training job</span>
huggingface_estimator_loaded<span style="color:#f92672">.</span>model_data
</code></pre></div><pre><code>2021-01-15 19:31:50 Starting - Preparing the instances for training
2021-01-15 19:31:50 Downloading - Downloading input data
2021-01-15 19:31:50 Training - Training image download completed. Training in progress.
2021-01-15 19:31:50 Uploading - Uploading generated training model
2021-01-15 19:31:50 Completed - Training job completed





's3://philipps-sagemaker-bucket-eu-central-1/huggingface-sdk-extension-2021-01-15-19-14-13-725/output/model.tar.gz'
</code></pre>


<footer class=" footline" >
	
</footer>


        
        </div>
        

      </div>

    <div id="navigation">
        
        

        
            
            
                
                    
                    
                
                

                    
                    
                        
                    
                    

                    
                        
            
            
                
                    
                    
                
                

                    
                    
                        
                    
                    

                    
                        
            
            
                
                    
                        
                        
                    
                
                

                    
                    
                    

                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
                        
            
            
                
                    
                    
                
                

                    
                    
                    

                    
                        
            
            
                
                    
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
            
        
                    
            
        

        


        
            <a class="nav nav-prev" href="../../../workshop/5_huggingface/" title="Use a custom Docker image"> <i class="fas fa-chevron-left"></i></a>
        
        
            <a class="nav nav-next" href="../../../workshop/1_gettingstarted/" title="Getting Started" style="margin-right: 0px;"><i class="fas fa-chevron-right"></i></a>
        
    </div>

    </section>

    <div style="left: -1000px; overflow: scroll; position: absolute; top: -1000px; border: none; box-sizing: content-box; height: 200px; margin: 0px; padding: 0px; width: 200px;">
      <div style="border: none; box-sizing: content-box; height: 200px; margin: 0px; padding: 0px; width: 200px;"></div>
    </div>
    <script src="../../../js/clipboard.min.js?1627670778"></script>
    <script src="../../../js/perfect-scrollbar.min.js?1627670778"></script>
    <script src="../../../js/perfect-scrollbar.jquery.min.js?1627670778"></script>
    <script src="../../../js/jquery.sticky.js?1627670778"></script>
    <script src="../../../js/featherlight.min.js?1627670778"></script>
    <script src="../../../js/html5shiv-printshiv.min.js?1627670778"></script>
    <script src="../../../js/highlight.pack.js?1627670778"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="../../../js/modernizr.custom.71422.js?1627670778"></script>
    <script src="../../../js/learn.js?1627670778"></script>
    <script src="../../../js/hugo-learn.js?1627670778"></script>

    <link href="../../../mermaid/mermaid.css?1627670778" type="text/css" rel="stylesheet" />
    <script src="../../../mermaid/mermaid.min.js?1627670778"></script>
    <script>
	var config = {
        startOnLoad:true,
        flowchart:{
	    curve:'basis'
        }
      };
        mermaid.initialize(config);
    </script>
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-44634850-4', 'auto');
	
	ga('send', 'pageview');
}
</script>

  </body>
</html>

