<!DOCTYPE html>
<html class="no-js" lang="en-US" prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#">
<html lang="en" class="js csstransforms3d">
  <head>
    <meta charset="utf-8">
    <meta property="og:title" content="RoboMakerWorkshops.com" />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://www.robomakerworkshops.com" />
    <meta property="og:image" content="https://www.robomakerworkshops.com/images/2_all_windows.png" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    <meta name="generator" content="Hugo 0.86.0" />
    <meta name="description" content="Amazon SageMaker Workshop">


    <link rel="shortcut icon" href="../../../images/favicon.ico" type="image/x-icon" />
<link rel="icon" href="../../../images/favicon.ico" type="image/x-icon" />

    <title> :: Get Started on Amazon SageMaker</title>

    
    <link href="../../../css/nucleus.css?1627670778" rel="stylesheet">
    <link href="../../../css/fontawesome-all.min.css?1627670778" rel="stylesheet">
    <link href="../../../css/hybrid.css?1627670778" rel="stylesheet">
    <link href="../../../css/featherlight.min.css?1627670778" rel="stylesheet">
    <link href="../../../css/perfect-scrollbar.min.css?1627670778" rel="stylesheet">
    <link href="../../../css/auto-complete.css?1627670778" rel="stylesheet">
    <link href="../../../css/theme.css?1627670778" rel="stylesheet">
    <link href="../../../css/hugo-theme.css?1627670778" rel="stylesheet">
    <link href="../../../css/jquery-ui.min.css?1627670778" rel="stylesheet">
    
      <link href="../../../css/theme-mine.css?1627670778" rel="stylesheet">
    

    <script src="../../../js/jquery-3.3.1.min.js?1627670778"></script>
    <script src="../../../js/jquery-ui-1.12.1.min.js?1627670778"></script>


    <style type="text/css">
      :root #header + #content > #left > #rlblock_left{
          display:none !important;
      }
      
        :not(pre) > code + span.copy-to-clipboard {
            display: none;
        }
      
    </style>
    
  </head>
  <body class="" data-url="../../../workshop/5_huggingface/hf_spot_notebook/">
    <nav id="sidebar" class="">



  <div id="header-wrapper">
    <div id="header">
      <a href="../../../" title="Go home"><svg id="Layer_1" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 30" width="60%" style="padding:20px 0px;"><defs><style>.cls-1{fill:#fff;}.cls-2{fill:#f90;fill-rule:evenodd;}</style></defs><title>AWS-Logo_White-Color</title><path class="cls-1" d="M14.09,10.85a4.7,4.7,0,0,0,.19,1.48,7.73,7.73,0,0,0,.54,1.19.77.77,0,0,1,.12.38.64.64,0,0,1-.32.49l-1,.7a.83.83,0,0,1-.44.15.69.69,0,0,1-.49-.23,3.8,3.8,0,0,1-.6-.77q-.25-.42-.51-1a6.14,6.14,0,0,1-4.89,2.3,4.54,4.54,0,0,1-3.32-1.19,4.27,4.27,0,0,1-1.22-3.2A4.28,4.28,0,0,1,3.61,7.75,6.06,6.06,0,0,1,7.69,6.46a12.47,12.47,0,0,1,1.76.13q.92.13,1.91.36V5.73a3.65,3.65,0,0,0-.79-2.66A3.81,3.81,0,0,0,7.86,2.3a7.71,7.71,0,0,0-1.79.22,12.78,12.78,0,0,0-1.79.57,4.55,4.55,0,0,1-.58.22l-.26,0q-.35,0-.35-.52V2a1.09,1.09,0,0,1,.12-.58,1.2,1.2,0,0,1,.47-.35A10.88,10.88,0,0,1,5.77.32,10.19,10.19,0,0,1,8.36,0a6,6,0,0,1,4.35,1.35,5.49,5.49,0,0,1,1.38,4.09ZM7.34,13.38a5.36,5.36,0,0,0,1.72-.31A3.63,3.63,0,0,0,10.63,12,2.62,2.62,0,0,0,11.19,11a5.63,5.63,0,0,0,.16-1.44v-.7a14.35,14.35,0,0,0-1.53-.28,12.37,12.37,0,0,0-1.56-.1,3.84,3.84,0,0,0-2.47.67A2.34,2.34,0,0,0,5,11a2.35,2.35,0,0,0,.61,1.76A2.4,2.4,0,0,0,7.34,13.38Zm13.35,1.8a1,1,0,0,1-.64-.16,1.3,1.3,0,0,1-.35-.65L15.81,1.51a3,3,0,0,1-.15-.67.36.36,0,0,1,.41-.41H17.7a1,1,0,0,1,.65.16,1.4,1.4,0,0,1,.33.65l2.79,11,2.59-11A1.17,1.17,0,0,1,24.39.6a1.1,1.1,0,0,1,.67-.16H26.4a1.1,1.1,0,0,1,.67.16,1.17,1.17,0,0,1,.32.65L30,12.39,32.88,1.25A1.39,1.39,0,0,1,33.22.6a1,1,0,0,1,.65-.16h1.54a.36.36,0,0,1,.41.41,1.36,1.36,0,0,1,0,.26,3.64,3.64,0,0,1-.12.41l-4,12.86a1.3,1.3,0,0,1-.35.65,1,1,0,0,1-.64.16H29.25a1,1,0,0,1-.67-.17,1.26,1.26,0,0,1-.32-.67L25.67,3.64,23.11,14.34a1.26,1.26,0,0,1-.32.67,1,1,0,0,1-.67.17Zm21.36.44a11.28,11.28,0,0,1-2.56-.29,7.44,7.44,0,0,1-1.92-.67,1,1,0,0,1-.61-.93v-.84q0-.52.38-.52a.9.9,0,0,1,.31.06l.42.17a8.77,8.77,0,0,0,1.83.58,9.78,9.78,0,0,0,2,.2,4.48,4.48,0,0,0,2.43-.55,1.76,1.76,0,0,0,.86-1.57,1.61,1.61,0,0,0-.45-1.16A4.29,4.29,0,0,0,43,9.22l-2.41-.76A5.15,5.15,0,0,1,38,6.78a3.94,3.94,0,0,1-.83-2.41,3.7,3.7,0,0,1,.45-1.85,4.47,4.47,0,0,1,1.19-1.37A5.27,5.27,0,0,1,40.51.29,7.4,7.4,0,0,1,42.6,0a8.87,8.87,0,0,1,1.12.07q.57.07,1.08.19t.95.26a4.27,4.27,0,0,1,.7.29,1.59,1.59,0,0,1,.49.41.94.94,0,0,1,.15.55v.79q0,.52-.38.52a1.76,1.76,0,0,1-.64-.2,7.74,7.74,0,0,0-3.2-.64,4.37,4.37,0,0,0-2.21.47,1.6,1.6,0,0,0-.79,1.48,1.58,1.58,0,0,0,.49,1.18,4.94,4.94,0,0,0,1.83.92L44.55,7a5.08,5.08,0,0,1,2.57,1.6A3.76,3.76,0,0,1,47.9,11a4.21,4.21,0,0,1-.44,1.93,4.4,4.4,0,0,1-1.21,1.47,5.43,5.43,0,0,1-1.85.93A8.25,8.25,0,0,1,42.05,15.62Z"></path><path class="cls-2" d="M45.19,23.81C39.72,27.85,31.78,30,25,30A36.64,36.64,0,0,1,.22,20.57c-.51-.46-.06-1.09.56-.74A49.78,49.78,0,0,0,25.53,26.4,49.23,49.23,0,0,0,44.4,22.53C45.32,22.14,46.1,23.14,45.19,23.81Z"></path><path class="cls-2" d="M47.47,21.21c-.7-.9-4.63-.42-6.39-.21-.53.06-.62-.4-.14-.74,3.13-2.2,8.27-1.57,8.86-.83s-.16,5.89-3.09,8.35c-.45.38-.88.18-.68-.32C46.69,25.8,48.17,22.11,47.47,21.21Z"></path></svg></a>

    </div>
    
        <div class="searchbox">
    <label for="search-by"><i class="fas fa-search"></i></label>
    <input data-search-input id="search-by" type="text" placeholder="Search...">
    <span data-search-clear=""><i class="fas fa-close"></i></span>
</div>

<script type="text/javascript" src="../../../js/lunr.min.js?1627670778"></script>
<script type="text/javascript" src="../../../js/auto-complete.js?1627670778"></script>
<script type="text/javascript">
    
        var baseurl = '\/';
    
</script>
<script type="text/javascript" src="../../../js/search.js?1627670778"></script>

    
  </div>

    <div class="highlightable">
    <ul class="topics">

        
          
          


 
  
    
    <li data-nav-id="/workshop/" title="NLP on AWS" class="dd-item 
        parent
        
        
        ">
      <a href="../../../workshop/">
          NLP on AWS
          
      </a>
      
      
        <ul>
          
          
            
          
          
          
        
          
            
            


 
  
    
    <li data-nav-id="/workshop/1_gettingstarted/" title="Getting Started" class="dd-item 
        
        
        
        ">
      <a href="../../../workshop/1_gettingstarted/">
          Getting Started
          
      </a>
      
      
        <ul>
          
          
          
          
        
          
            
            


 
  
    
      <li data-nav-id="/workshop/1_gettingstarted/create/" title="Setup" class="dd-item ">
        <a href="../../../workshop/1_gettingstarted/create/">
        Setup
        
        </a>
    </li>
     
  
 

            
          
            
            


 
  
    
      <li data-nav-id="/workshop/1_gettingstarted/clone/" title="Clone the Repository" class="dd-item ">
        <a href="../../../workshop/1_gettingstarted/clone/">
        Clone the Repository
        
        </a>
    </li>
     
  
 

            
          
        
        </ul>
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/workshop/2_builtin/" title="Built-in Algorithms" class="dd-item 
        
        
        
        ">
      <a href="../../../workshop/2_builtin/">
          Built-in Algorithms
          
      </a>
      
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/workshop/3_scriptmode/" title="SageMaker Script Mode" class="dd-item 
        
        
        
        ">
      <a href="../../../workshop/3_scriptmode/">
          SageMaker Script Mode
          
      </a>
      
      
        <ul>
          
          
          
          
        
          
            
            


 
  
    
      <li data-nav-id="/workshop/3_scriptmode/pytorch_rnn/" title="Word-level language modeling using PyTorch" class="dd-item ">
        <a href="../../../workshop/3_scriptmode/pytorch_rnn/">
        Word-level language modeling using PyTorch
        
        </a>
    </li>
     
  
 

            
          
        
        </ul>
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/workshop/4_byoc/" title="Hugging Face" class="dd-item 
        
        
        
        ">
      <a href="../../../workshop/4_byoc/">
          Hugging Face
          
      </a>
      
      
        <ul>
          
          
          
          
        
          
            
            


 
  
    
      <li data-nav-id="/workshop/4_byoc/pytorch_rnn/" title="Word-level language modeling using PyTorch" class="dd-item ">
        <a href="../../../workshop/4_byoc/pytorch_rnn/">
        Word-level language modeling using PyTorch
        
        </a>
    </li>
     
  
 

            
          
        
        </ul>
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/workshop/5_huggingface/" title="Use a custom Docker image" class="dd-item 
        parent
        
        
        ">
      <a href="../../../workshop/5_huggingface/">
          Use a custom Docker image
          
      </a>
      
      
        <ul>
          
          
          
          
        
          
            
            


 
  
    
      <li data-nav-id="/workshop/5_huggingface/hf_spot_notebook/" title="" class="dd-item active">
        <a href="../../../workshop/5_huggingface/hf_spot_notebook/">
        
        
        </a>
    </li>
     
  
 

            
          
        
        </ul>
              
    </li>
  
 

            
          
        
        </ul>
              
    </li>
  
 

          
         
    </ul>

    
    
      <section id="shortcuts">
        <h3>More</h3>
        <ul>
          
              <li> 
                  <a class="padding" href="../../../more_resources"><i class='fas fa-bookmark'></i> More Resources</a>
              </li>
          
              <li> 
                  <a class="padding" href="../../../authors"><i class='fas fa-users'></i> Authors</a>
              </li>
          
        </ul>
      </section>
    

    
    <section id="footer">
      <left>

    <h2 class="github-title">Amazon SageMaker</h2>
    <h5 class="copyright">&copy; 2020 Amazon Web Services, Inc. or its Affiliates. All rights reserved.<h5>

</left>

<script async defer src="https://buttons.github.io/buttons.js"></script>

    </section>
  </div>
</nav>





        <section id="body">
        <div id="overlay"></div>
        <div class="padding highlightable">
              
              <div>
                <div id="top-bar">
                
                  
                  
                  
                  <div id="top-github-link">
                    <a class="github-link" title='Edit this page' href="https://github.com/w601sxs/aws-sagemaker-workshop/edit/master/content/workshop/5_HuggingFace/HF_spot_notebook.md" target="blank">
                      <i class="fas fa-code-branch"></i>
                      <span id="top-github-link-text">Edit this page</span>
                    </a>
                  </div>
                  
                
                
                <div id="breadcrumbs" itemscope="" itemtype="http://data-vocabulary.org/Breadcrumb">
                    <span id="sidebar-toggle-span">
                        <a href="#" id="sidebar-toggle" data-sidebar-toggle="">
                          <i class="fas fa-bars"></i>
                        </a>
                    </span>
                  
                  <span id="toc-menu"><i class="fas fa-list-alt"></i></span>
                  
                  <span class="links">
                    
          
          
            
            
          
          
            
            
          
          
            
            
          
          
            <a href='../../../'>NLP Workshop</a> > <a href='../../../workshop/'>NLP on AWS</a> > <a href='../../../workshop/5_huggingface/'>Use a custom Docker image</a> > 
          
         
          
         
          
         
          
           
                  </span>
                </div>
                
                    <div class="progress">
    <div class="wrapper">
<nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#binary-classification-with-trainer-and-imdb-dataset">Binary Classification with <code>Trainer</code> and <code>imdb</code> dataset</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#installation">Installation</a></li>
    <li><a href="#development-environment">Development environment</a></li>
    <li><a href="#permissions">Permissions</a></li>
  </ul>

  <ul>
    <li><a href="#tokenization">Tokenization</a></li>
    <li><a href="#uploading-data-to-sagemaker_session_bucket">Uploading data to <code>sagemaker_session_bucket</code></a></li>
  </ul>

  <ul>
    <li><a href="#creating-an-estimator-and-start-a-training-job">Creating an Estimator and start a training job</a></li>
    <li><a href="#deploying-the-endpoint">Deploying the endpoint</a></li>
  </ul>

  <ul>
    <li><a href="#estimator-parameters">Estimator Parameters</a></li>
    <li><a href="#attach-to-old-training-job-to-an-estimator">Attach to old training job to an estimator</a></li>
  </ul>
</nav>
    </div>
</div>

                
              </div>
            </div>
            

        
        <div id="body-inner">
          
            
          

        


<h1></h1>

<h1 id="huggingface-sagemaker-sdk---spot-instances-example">Huggingface Sagemaker-sdk - Spot instances example</h1>
<h3 id="binary-classification-with-trainer-and-imdb-dataset">Binary Classification with <code>Trainer</code> and <code>imdb</code> dataset</h3>
<ol>
<li><a href="#Introduction">Introduction</a></li>
<li><a href="#Development-Environment-and-Permissions">Development Environment and Permissions</a>
<ol>
<li><a href="#Installation">Installation</a></li>
<li><a href="#Development-environment">Development environment</a></li>
<li><a href="#Permissions">Permissions</a></li>
</ol>
</li>
<li><a href="#Preprocessing">Processing</a>
<ol>
<li><a href="#Tokenization">Tokenization</a></li>
<li><a href="#Uploading-data-to-sagemaker_session_bucket">Uploading data to sagemaker_session_bucket</a></li>
</ol>
</li>
<li><a href="#Fine-tuning-&amp;-starting-Sagemaker-Training-Job">Fine-tuning &amp; starting Sagemaker Training Job</a>
<ol>
<li><a href="#Creating-an-Estimator-and-start-a-training-job">Creating an Estimator and start a training job</a></li>
<li><a href="#Estimator-Parameters">Estimator Parameters</a></li>
<li><a href="#Download-fine-tuned-model-from-s3">Download fine-tuned model from s3</a></li>
<li><a href="#Attach-to-old-training-job-to-an-estimator">Attach to old training job to an estimator </a></li>
</ol>
</li>
<li><a href="#Push-model-to-the-Hugging-Face-hub"><em>Coming soon</em>:Push model to the Hugging Face hub</a></li>
</ol>
<h1 id="introduction">Introduction</h1>
<p>Welcome to our end-to-end binary Text-Classification example. In this demo, we will use the Hugging Faces <code>transformers</code> and <code>datasets</code> library together with a custom Amazon sagemaker-sdk extension to fine-tune a pre-trained transformer on binary text classification. In particular, the pre-trained model will be fine-tuned using the <code>imdb</code> dataset. To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on. This demo will also show you can use spot instances and continue training.</p>
<p><img src="attachment:image.png" alt="image.png"></p>
<p><em><strong>NOTE: You can run this demo in Sagemaker Studio, your local machine or Sagemaker Notebook Instances</strong></em></p>
<h1 id="development-environment-and-permissions">Development Environment and Permissions</h1>
<h2 id="installation">Installation</h2>
<p><em><em>Note:</em> we only install the required libraries from Hugging Face and AWS. You also need PyTorch or Tensorflow, if you havenÂ´t it installed</em></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#960050;background-color:#1e0010">!</span>pip install <span style="color:#e6db74">&#34;sagemaker&gt;=2.48.0&#34;</span> <span style="color:#e6db74">&#34;transformers==4.6.1&#34;</span> <span style="color:#e6db74">&#34;datasets[s3]==1.6.2&#34;</span> <span style="color:#f92672">--</span>upgrade
</code></pre></div><h2 id="development-environment">Development environment</h2>
<p><strong>upgrade ipywidgets for <code>datasets</code> library and restart kernel, only needed when prerpocessing is done in the notebook</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">%%</span>capture
<span style="color:#f92672">import</span> IPython
<span style="color:#960050;background-color:#1e0010">!</span>conda install <span style="color:#f92672">-</span>c conda<span style="color:#f92672">-</span>forge ipywidgets <span style="color:#f92672">-</span>y
IPython<span style="color:#f92672">.</span>Application<span style="color:#f92672">.</span>instance()<span style="color:#f92672">.</span>kernel<span style="color:#f92672">.</span>do_shutdown(<span style="color:#66d9ef">True</span>) <span style="color:#75715e"># has to restart kernel so changes are used</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> sagemaker.huggingface
</code></pre></div><h2 id="permissions">Permissions</h2>
<p><em>If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html">here</a> more about it.</em></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> sagemaker

sess <span style="color:#f92672">=</span> sagemaker<span style="color:#f92672">.</span>Session()
<span style="color:#75715e"># sagemaker session bucket -&gt; used for uploading data, models and logs</span>
<span style="color:#75715e"># sagemaker will automatically create this bucket if it not exists</span>
sagemaker_session_bucket<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>
<span style="color:#66d9ef">if</span> sagemaker_session_bucket <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span> <span style="color:#f92672">and</span> sess <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
    <span style="color:#75715e"># set to default bucket if a bucket name is not given</span>
    sagemaker_session_bucket <span style="color:#f92672">=</span> sess<span style="color:#f92672">.</span>default_bucket()

role <span style="color:#f92672">=</span> sagemaker<span style="color:#f92672">.</span>get_execution_role()
sess <span style="color:#f92672">=</span> sagemaker<span style="color:#f92672">.</span>Session(default_bucket<span style="color:#f92672">=</span>sagemaker_session_bucket)

print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;sagemaker role arn: </span><span style="color:#e6db74">{</span>role<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;sagemaker bucket: </span><span style="color:#e6db74">{</span>sess<span style="color:#f92672">.</span>default_bucket()<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;sagemaker session region: </span><span style="color:#e6db74">{</span>sess<span style="color:#f92672">.</span>boto_region_name<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</code></pre></div><h1 id="preprocessing">Preprocessing</h1>
<p>We are using the <code>datasets</code> library to download and preprocess the <code>imdb</code> dataset. After preprocessing, the dataset will be uploaded to our <code>sagemaker_session_bucket</code> to be used within our training job. The <a href="http://ai.stanford.edu/~amaas/data/sentiment/">imdb</a> dataset consists of 25000 training and 25000 testing highly polar movie reviews.</p>
<h2 id="tokenization">Tokenization</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> datasets <span style="color:#f92672">import</span> load_dataset
<span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoTokenizer

<span style="color:#75715e"># tokenizer used in preprocessing</span>
tokenizer_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;distilbert-base-uncased&#39;</span>

<span style="color:#75715e"># dataset used</span>
dataset_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;imdb&#39;</span>

<span style="color:#75715e"># s3 key prefix for the data</span>
s3_prefix <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;samples/datasets/imdb&#39;</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># load dataset</span>
dataset <span style="color:#f92672">=</span> load_dataset(dataset_name)

<span style="color:#75715e"># download tokenizer</span>
tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(tokenizer_name)

<span style="color:#75715e"># tokenizer helper function</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">tokenize</span>(batch):
    <span style="color:#66d9ef">return</span> tokenizer(batch[<span style="color:#e6db74">&#39;text&#39;</span>], padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;max_length&#39;</span>, truncation<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)

<span style="color:#75715e"># load dataset</span>
train_dataset, test_dataset <span style="color:#f92672">=</span> load_dataset(<span style="color:#e6db74">&#39;imdb&#39;</span>, split<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;train&#39;</span>, <span style="color:#e6db74">&#39;test&#39;</span>])
test_dataset <span style="color:#f92672">=</span> test_dataset<span style="color:#f92672">.</span>shuffle()<span style="color:#f92672">.</span>select(range(<span style="color:#ae81ff">10000</span>)) <span style="color:#75715e"># smaller the size for test dataset to 10k </span>


<span style="color:#75715e"># tokenize dataset</span>
train_dataset <span style="color:#f92672">=</span> train_dataset<span style="color:#f92672">.</span>map(tokenize, batched<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
test_dataset <span style="color:#f92672">=</span> test_dataset<span style="color:#f92672">.</span>map(tokenize, batched<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)

<span style="color:#75715e"># set format for pytorch</span>
train_dataset <span style="color:#f92672">=</span>  train_dataset<span style="color:#f92672">.</span>rename_column(<span style="color:#e6db74">&#34;label&#34;</span>, <span style="color:#e6db74">&#34;labels&#34;</span>)
train_dataset<span style="color:#f92672">.</span>set_format(<span style="color:#e6db74">&#39;torch&#39;</span>, columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;input_ids&#39;</span>, <span style="color:#e6db74">&#39;attention_mask&#39;</span>, <span style="color:#e6db74">&#39;labels&#39;</span>])
test_dataset <span style="color:#f92672">=</span> test_dataset<span style="color:#f92672">.</span>rename_column(<span style="color:#e6db74">&#34;label&#34;</span>, <span style="color:#e6db74">&#34;labels&#34;</span>)
test_dataset<span style="color:#f92672">.</span>set_format(<span style="color:#e6db74">&#39;torch&#39;</span>, columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;input_ids&#39;</span>, <span style="color:#e6db74">&#39;attention_mask&#39;</span>, <span style="color:#e6db74">&#39;labels&#39;</span>])
</code></pre></div><h2 id="uploading-data-to-sagemaker_session_bucket">Uploading data to <code>sagemaker_session_bucket</code></h2>
<p>After we processed the <code>datasets</code> we are going to use the new <code>FileSystem</code> <a href="https://huggingface.co/docs/datasets/filesystems.html">integration</a> to upload our dataset to S3.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> botocore
<span style="color:#f92672">from</span> datasets.filesystems <span style="color:#f92672">import</span> S3FileSystem

s3 <span style="color:#f92672">=</span> S3FileSystem()  

<span style="color:#75715e"># save train_dataset to s3</span>
training_input_path <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;s3://</span><span style="color:#e6db74">{</span>sess<span style="color:#f92672">.</span>default_bucket()<span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">{</span>s3_prefix<span style="color:#e6db74">}</span><span style="color:#e6db74">/train&#39;</span>
train_dataset<span style="color:#f92672">.</span>save_to_disk(training_input_path,fs<span style="color:#f92672">=</span>s3)

<span style="color:#75715e"># save test_dataset to s3</span>
test_input_path <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;s3://</span><span style="color:#e6db74">{</span>sess<span style="color:#f92672">.</span>default_bucket()<span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">{</span>s3_prefix<span style="color:#e6db74">}</span><span style="color:#e6db74">/test&#39;</span>
test_dataset<span style="color:#f92672">.</span>save_to_disk(test_input_path,fs<span style="color:#f92672">=</span>s3)

</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">training_input_path <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;s3://</span><span style="color:#e6db74">{</span>sess<span style="color:#f92672">.</span>default_bucket()<span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">{</span>s3_prefix<span style="color:#e6db74">}</span><span style="color:#e6db74">/train&#39;</span>
test_input_path <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;s3://</span><span style="color:#e6db74">{</span>sess<span style="color:#f92672">.</span>default_bucket()<span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">{</span>s3_prefix<span style="color:#e6db74">}</span><span style="color:#e6db74">/test&#39;</span>

</code></pre></div><h1 id="fine-tuning--starting-sagemaker-training-job">Fine-tuning &amp; starting Sagemaker Training Job</h1>
<p>In order to create a sagemaker training job we need an <code>HuggingFace</code> Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as <code>entry_point</code>, which <code>instance_type</code> should be used, which <code>hyperparameters</code> are passed in &hellip;..</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">huggingface_estimator <span style="color:#f92672">=</span> HuggingFace(entry_point<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;train.py&#39;</span>,
                            source_dir<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;./scripts&#39;</span>,
                            base_job_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;huggingface-sdk-extension&#39;</span>,
                            instance_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;ml.p3.2xlarge&#39;</span>,
                            instance_count<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,
                            transformers_version<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;4.4&#39;</span>,
                            pytorch_version<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;1.6&#39;</span>,
                            py_version<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;py36&#39;</span>,
                            role<span style="color:#f92672">=</span>role,
                            hyperparameters <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;epochs&#39;</span>: <span style="color:#ae81ff">1</span>,
                                               <span style="color:#e6db74">&#39;train_batch_size&#39;</span>: <span style="color:#ae81ff">32</span>,
                                               <span style="color:#e6db74">&#39;model_name&#39;</span>:<span style="color:#e6db74">&#39;distilbert-base-uncased&#39;</span>
                                                })
</code></pre></div><p>When we create a SageMaker training job, SageMaker takes care of starting and managing all the required ec2 instances for us with the <code>huggingface</code> container, uploads the provided fine-tuning script <code>train.py</code> and downloads the data from our <code>sagemaker_session_bucket</code> into the container at <code>/opt/ml/input/data</code>. Then, it starts the training job by running.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">/</span>opt<span style="color:#f92672">/</span>conda<span style="color:#f92672">/</span>bin<span style="color:#f92672">/</span>python train<span style="color:#f92672">.</span>py <span style="color:#f92672">--</span>epochs <span style="color:#ae81ff">1</span> <span style="color:#f92672">--</span>model_name distilbert<span style="color:#f92672">-</span>base<span style="color:#f92672">-</span>uncased <span style="color:#f92672">--</span>train_batch_size <span style="color:#ae81ff">32</span>
</code></pre></div><p>The <code>hyperparameters</code> you define in the <code>HuggingFace</code> estimator are passed in as named arguments.</p>
<p>Sagemaker is providing useful properties about the training environment through various environment variables, including the following:</p>
<ul>
<li>
<p><code>SM_MODEL_DIR</code>: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting.</p>
</li>
<li>
<p><code>SM_NUM_GPUS</code>: An integer representing the number of GPUs available to the host.</p>
</li>
<li>
<p><code>SM_CHANNEL_XXXX:</code> A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimatorâ€™s fit call, named <code>train</code> and <code>test</code>, the environment variables <code>SM_CHANNEL_TRAIN</code> and <code>SM_CHANNEL_TEST</code> are set.</p>
</li>
</ul>
<p>To run your training job locally you can define <code>instance_type='local'</code> or <code>instance_type='local-gpu'</code> for gpu usage. <em>Note: this does not working within SageMaker Studio</em></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#960050;background-color:#1e0010">!</span>pygmentize <span style="color:#f92672">./</span>scripts<span style="color:#f92672">/</span>train<span style="color:#f92672">.</span>py
</code></pre></div><pre><code>[34mfrom[39;49;00m [04m[36mtransformers[39;49;00m [34mimport[39;49;00m AutoModelForSequenceClassification, Trainer, TrainingArguments
[34mfrom[39;49;00m [04m[36mtransformers[39;49;00m[04m[36m.[39;49;00m[04m[36mtrainer_utils[39;49;00m [34mimport[39;49;00m get_last_checkpoint

[34mfrom[39;49;00m [04m[36msklearn[39;49;00m[04m[36m.[39;49;00m[04m[36mmetrics[39;49;00m [34mimport[39;49;00m accuracy_score, precision_recall_fscore_support
[34mfrom[39;49;00m [04m[36mdatasets[39;49;00m [34mimport[39;49;00m load_from_disk
[34mimport[39;49;00m [04m[36mlogging[39;49;00m
[34mimport[39;49;00m [04m[36msys[39;49;00m
[34mimport[39;49;00m [04m[36margparse[39;49;00m
[34mimport[39;49;00m [04m[36mos[39;49;00m

[37m# Set up logging[39;49;00m
logger = logging.getLogger([31m__name__[39;49;00m)

logging.basicConfig(
    level=logging.getLevelName([33m&quot;[39;49;00m[33mINFO[39;49;00m[33m&quot;[39;49;00m),
    handlers=[logging.StreamHandler(sys.stdout)],
    [36mformat[39;49;00m=[33m&quot;[39;49;00m[33m%(asctime)s[39;49;00m[33m - [39;49;00m[33m%(name)s[39;49;00m[33m - [39;49;00m[33m%(levelname)s[39;49;00m[33m - [39;49;00m[33m%(message)s[39;49;00m[33m&quot;[39;49;00m,
)

[34mif[39;49;00m [31m__name__[39;49;00m == [33m&quot;[39;49;00m[33m__main__[39;49;00m[33m&quot;[39;49;00m:

    logger.info(sys.argv)

    parser = argparse.ArgumentParser()

    [37m# hyperparameters sent by the client are passed as command-line arguments to the script.[39;49;00m
    parser.add_argument([33m&quot;[39;49;00m[33m--epochs[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mint[39;49;00m, default=[34m3[39;49;00m)
    parser.add_argument([33m&quot;[39;49;00m[33m--train-batch-size[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mint[39;49;00m, default=[34m32[39;49;00m)
    parser.add_argument([33m&quot;[39;49;00m[33m--eval-batch-size[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mint[39;49;00m, default=[34m64[39;49;00m)
    parser.add_argument([33m&quot;[39;49;00m[33m--warmup_steps[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mint[39;49;00m, default=[34m500[39;49;00m)
    parser.add_argument([33m&quot;[39;49;00m[33m--model_name[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mstr[39;49;00m)
    parser.add_argument([33m&quot;[39;49;00m[33m--learning_rate[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mstr[39;49;00m, default=[34m5e-5[39;49;00m)
    parser.add_argument([33m&quot;[39;49;00m[33m--output_dir[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mstr[39;49;00m)

    [37m# Data, model, and output directories[39;49;00m
    parser.add_argument([33m&quot;[39;49;00m[33m--output-data-dir[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mstr[39;49;00m, default=os.environ[[33m&quot;[39;49;00m[33mSM_OUTPUT_DATA_DIR[39;49;00m[33m&quot;[39;49;00m])
    parser.add_argument([33m&quot;[39;49;00m[33m--model-dir[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mstr[39;49;00m, default=os.environ[[33m&quot;[39;49;00m[33mSM_MODEL_DIR[39;49;00m[33m&quot;[39;49;00m])
    parser.add_argument([33m&quot;[39;49;00m[33m--n_gpus[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mstr[39;49;00m, default=os.environ[[33m&quot;[39;49;00m[33mSM_NUM_GPUS[39;49;00m[33m&quot;[39;49;00m])
    parser.add_argument([33m&quot;[39;49;00m[33m--training_dir[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mstr[39;49;00m, default=os.environ[[33m&quot;[39;49;00m[33mSM_CHANNEL_TRAIN[39;49;00m[33m&quot;[39;49;00m])
    parser.add_argument([33m&quot;[39;49;00m[33m--test_dir[39;49;00m[33m&quot;[39;49;00m, [36mtype[39;49;00m=[36mstr[39;49;00m, default=os.environ[[33m&quot;[39;49;00m[33mSM_CHANNEL_TEST[39;49;00m[33m&quot;[39;49;00m])

    args, _ = parser.parse_known_args()

    [37m# load datasets[39;49;00m
    train_dataset = load_from_disk(args.training_dir)
    test_dataset = load_from_disk(args.test_dir)

    logger.info([33mf[39;49;00m[33m&quot;[39;49;00m[33m loaded train_dataset length is: [39;49;00m[33m{[39;49;00m[36mlen[39;49;00m(train_dataset)[33m}[39;49;00m[33m&quot;[39;49;00m)
    logger.info([33mf[39;49;00m[33m&quot;[39;49;00m[33m loaded test_dataset length is: [39;49;00m[33m{[39;49;00m[36mlen[39;49;00m(test_dataset)[33m}[39;49;00m[33m&quot;[39;49;00m)

    [37m# compute metrics function for binary classification[39;49;00m
    [34mdef[39;49;00m [32mcompute_metrics[39;49;00m(pred):
        labels = pred.label_ids
        preds = pred.predictions.argmax(-[34m1[39;49;00m)
        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=[33m&quot;[39;49;00m[33mbinary[39;49;00m[33m&quot;[39;49;00m)
        acc = accuracy_score(labels, preds)
        [34mreturn[39;49;00m {[33m&quot;[39;49;00m[33maccuracy[39;49;00m[33m&quot;[39;49;00m: acc, [33m&quot;[39;49;00m[33mf1[39;49;00m[33m&quot;[39;49;00m: f1, [33m&quot;[39;49;00m[33mprecision[39;49;00m[33m&quot;[39;49;00m: precision, [33m&quot;[39;49;00m[33mrecall[39;49;00m[33m&quot;[39;49;00m: recall}

    [37m# download model from model hub[39;49;00m
    model = AutoModelForSequenceClassification.from_pretrained(args.model_name)

    [37m# define training args[39;49;00m
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.train_batch_size,
        per_device_eval_batch_size=args.eval_batch_size,
        warmup_steps=args.warmup_steps,
        evaluation_strategy=[33m&quot;[39;49;00m[33mepoch[39;49;00m[33m&quot;[39;49;00m,
        logging_dir=[33mf[39;49;00m[33m&quot;[39;49;00m[33m{[39;49;00margs.output_data_dir[33m}[39;49;00m[33m/logs[39;49;00m[33m&quot;[39;49;00m,
        learning_rate=[36mfloat[39;49;00m(args.learning_rate),
    )

    [37m# create Trainer instance[39;49;00m
    trainer = Trainer(
        model=model,
        args=training_args,
        compute_metrics=compute_metrics,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
    )

    [37m# train model[39;49;00m
    [34mif[39;49;00m get_last_checkpoint(args.output_dir) [35mis[39;49;00m [35mnot[39;49;00m [34mNone[39;49;00m:
        logger.info([33m&quot;[39;49;00m[33m***** continue training *****[39;49;00m[33m&quot;[39;49;00m)
        trainer.train(resume_from_checkpoint=args.output_dir)
    [34melse[39;49;00m:
        trainer.train()
    [37m# evaluate model[39;49;00m
    eval_result = trainer.evaluate(eval_dataset=test_dataset)

    [37m# writes eval result to file which can be accessed later in s3 ouput[39;49;00m
    [34mwith[39;49;00m [36mopen[39;49;00m(os.path.join(args.output_data_dir, [33m&quot;[39;49;00m[33meval_results.txt[39;49;00m[33m&quot;[39;49;00m), [33m&quot;[39;49;00m[33mw[39;49;00m[33m&quot;[39;49;00m) [34mas[39;49;00m writer:
        [36mprint[39;49;00m([33mf[39;49;00m[33m&quot;[39;49;00m[33m***** Eval results *****[39;49;00m[33m&quot;[39;49;00m)
        [34mfor[39;49;00m key, value [35min[39;49;00m [36msorted[39;49;00m(eval_result.items()):
            writer.write([33mf[39;49;00m[33m&quot;[39;49;00m[33m{[39;49;00mkey[33m}[39;49;00m[33m = [39;49;00m[33m{[39;49;00mvalue[33m}[39;49;00m[33m\n[39;49;00m[33m&quot;[39;49;00m)

    [37m# Saves the model to s3[39;49;00m
    trainer.save_model(args.model_dir)
</code></pre>
<h2 id="creating-an-estimator-and-start-a-training-job">Creating an Estimator and start a training job</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sagemaker.huggingface <span style="color:#f92672">import</span> HuggingFace

<span style="color:#75715e"># hyperparameters, which are passed into the training job</span>
hyperparameters<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#39;epochs&#39;</span>: <span style="color:#ae81ff">1</span>,
                 <span style="color:#e6db74">&#39;train_batch_size&#39;</span>: <span style="color:#ae81ff">32</span>,
                 <span style="color:#e6db74">&#39;model_name&#39;</span>:<span style="color:#e6db74">&#39;distilbert-base-uncased&#39;</span>,
                 <span style="color:#e6db74">&#39;output_dir&#39;</span>:<span style="color:#e6db74">&#39;/opt/ml/checkpoints&#39;</span>
                 }

<span style="color:#75715e"># s3 uri where our checkpoints will be uploaded during training</span>
job_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;using-spot&#34;</span>
checkpoint_s3_uri <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;s3://</span><span style="color:#e6db74">{</span>sess<span style="color:#f92672">.</span>default_bucket()<span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">{</span>job_name<span style="color:#e6db74">}</span><span style="color:#e6db74">/checkpoints&#39;</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">huggingface_estimator <span style="color:#f92672">=</span> HuggingFace(entry_point<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;train.py&#39;</span>,
                            source_dir<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;./scripts&#39;</span>,
                            instance_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;ml.p3.2xlarge&#39;</span>,
                            instance_count<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,
                            base_job_name<span style="color:#f92672">=</span>job_name,
                            checkpoint_s3_uri<span style="color:#f92672">=</span>checkpoint_s3_uri,
                            use_spot_instances<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
                            max_wait<span style="color:#f92672">=</span><span style="color:#ae81ff">3600</span>, <span style="color:#75715e"># This should be equal to or greater than max_run in seconds&#39;</span>
                            max_run<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, <span style="color:#75715e"># expected max run in seconds</span>
                            role<span style="color:#f92672">=</span>role,
                            transformers_version<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;4.6&#39;</span>,
                            pytorch_version<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;1.7&#39;</span>,
                            py_version<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;py36&#39;</span>,
                            hyperparameters <span style="color:#f92672">=</span> hyperparameters)
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># starting the train job with our uploaded datasets as input</span>
huggingface_estimator<span style="color:#f92672">.</span>fit({<span style="color:#e6db74">&#39;train&#39;</span>: training_input_path, <span style="color:#e6db74">&#39;test&#39;</span>: test_input_path})

<span style="color:#75715e"># Training seconds: 874</span>
<span style="color:#75715e"># Billable seconds: 262</span>
<span style="color:#75715e"># Managed Spot Training savings: 70.0%</span>
</code></pre></div><h2 id="deploying-the-endpoint">Deploying the endpoint</h2>
<p>To deploy our endpoint, we call <code>deploy()</code> on our HuggingFace estimator object, passing in our desired number of instances and instance type.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">predictor <span style="color:#f92672">=</span> huggingface_estimator<span style="color:#f92672">.</span>deploy(<span style="color:#ae81ff">1</span>,<span style="color:#e6db74">&#34;ml.g4dn.xlarge&#34;</span>)
</code></pre></div><p>Then, we use the returned predictor object to call the endpoint.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">sentiment_input<span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;inputs&#34;</span>:<span style="color:#e6db74">&#34;I love using the new Inference DLC.&#34;</span>}

predictor<span style="color:#f92672">.</span>predict(sentiment_input)
</code></pre></div><p>Finally, we delete the endpoint again.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">predictor<span style="color:#f92672">.</span>delete_endpoint()
</code></pre></div><h1 id="extras">Extras</h1>
<h2 id="estimator-parameters">Estimator Parameters</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># container image used for training job</span>
print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;container image used for training job: </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">{</span>huggingface_estimator<span style="color:#f92672">.</span>image_uri<span style="color:#e6db74">}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)

<span style="color:#75715e"># s3 uri where the trained model is located</span>
print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;s3 uri where the trained model is located: </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">{</span>huggingface_estimator<span style="color:#f92672">.</span>model_data<span style="color:#e6db74">}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)

<span style="color:#75715e"># latest training job name for this estimator</span>
print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;latest training job name for this estimator: </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">{</span>huggingface_estimator<span style="color:#f92672">.</span>latest_training_job<span style="color:#f92672">.</span>name<span style="color:#e6db74">}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)


</code></pre></div><pre><code>container image used for training job: 
558105141721.dkr.ecr.us-east-1.amazonaws.com/huggingface-training:pytorch1.6.0-transformers4.2.2-tokenizers0.9.4-datasets1.2.1-py36-gpu-cu110

s3 uri where the trained model is located: 
s3://philipps-sagemaker-bucket-us-east-1/huggingface-training-2021-02-04-16-47-39-189/output/model.tar.gz

latest training job name for this estimator: 
huggingface-training-2021-02-04-16-47-39-189
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># access the logs of the training job</span>
huggingface_estimator<span style="color:#f92672">.</span>sagemaker_session<span style="color:#f92672">.</span>logs_for_job(huggingface_estimator<span style="color:#f92672">.</span>latest_training_job<span style="color:#f92672">.</span>name)
</code></pre></div><h2 id="attach-to-old-training-job-to-an-estimator">Attach to old training job to an estimator</h2>
<p>In Sagemaker you can attach an old training job to an estimator to continue training, get results etc..</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sagemaker.estimator <span style="color:#f92672">import</span> Estimator

<span style="color:#75715e"># job which is going to be attached to the estimator</span>
old_training_job_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;&#39;</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># attach old training job</span>
huggingface_estimator_loaded <span style="color:#f92672">=</span> Estimator<span style="color:#f92672">.</span>attach(old_training_job_name)

<span style="color:#75715e"># get model output s3 from training job</span>
huggingface_estimator_loaded<span style="color:#f92672">.</span>model_data
</code></pre></div><pre><code>2021-01-15 19:31:50 Starting - Preparing the instances for training
2021-01-15 19:31:50 Downloading - Downloading input data
2021-01-15 19:31:50 Training - Training image download completed. Training in progress.
2021-01-15 19:31:50 Uploading - Uploading generated training model
2021-01-15 19:31:50 Completed - Training job completed





's3://philipps-sagemaker-bucket-eu-central-1/huggingface-sdk-extension-2021-01-15-19-14-13-725/output/model.tar.gz'
</code></pre>


<footer class=" footline" >
	
</footer>


        
        </div>
        

      </div>

    <div id="navigation">
        
        

        
            
            
                
                    
                    
                
                

                    
                    
                        
                    
                    

                    
                        
            
            
                
                    
                    
                
                

                    
                    
                        
                    
                    

                    
                        
            
            
                
                    
                        
                        
                    
                
                

                    
                    
                    

                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
                        
            
            
                
                    
                    
                
                

                    
                    
                    

                    
                        
            
            
                
                    
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
            
        
                    
            
        

        


        
            <a class="nav nav-prev" href="../../../workshop/5_huggingface/" title="Use a custom Docker image"> <i class="fas fa-chevron-left"></i></a>
        
        
            <a class="nav nav-next" href="../../../workshop/1_gettingstarted/" title="Getting Started" style="margin-right: 0px;"><i class="fas fa-chevron-right"></i></a>
        
    </div>

    </section>

    <div style="left: -1000px; overflow: scroll; position: absolute; top: -1000px; border: none; box-sizing: content-box; height: 200px; margin: 0px; padding: 0px; width: 200px;">
      <div style="border: none; box-sizing: content-box; height: 200px; margin: 0px; padding: 0px; width: 200px;"></div>
    </div>
    <script src="../../../js/clipboard.min.js?1627670778"></script>
    <script src="../../../js/perfect-scrollbar.min.js?1627670778"></script>
    <script src="../../../js/perfect-scrollbar.jquery.min.js?1627670778"></script>
    <script src="../../../js/jquery.sticky.js?1627670778"></script>
    <script src="../../../js/featherlight.min.js?1627670778"></script>
    <script src="../../../js/html5shiv-printshiv.min.js?1627670778"></script>
    <script src="../../../js/highlight.pack.js?1627670778"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="../../../js/modernizr.custom.71422.js?1627670778"></script>
    <script src="../../../js/learn.js?1627670778"></script>
    <script src="../../../js/hugo-learn.js?1627670778"></script>

    <link href="../../../mermaid/mermaid.css?1627670778" type="text/css" rel="stylesheet" />
    <script src="../../../mermaid/mermaid.min.js?1627670778"></script>
    <script>
	var config = {
        startOnLoad:true,
        flowchart:{
	    curve:'basis'
        }
      };
        mermaid.initialize(config);
    </script>
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-44634850-4', 'auto');
	
	ga('send', 'pageview');
}
</script>

  </body>
</html>

